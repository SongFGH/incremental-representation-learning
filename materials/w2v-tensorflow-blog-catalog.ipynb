{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/Ganymedian/Desktop/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import collections\n",
    "import zipfile\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from tempfile import gettempdir\n",
    "from sklearn import model_selection, linear_model, preprocessing, svm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    " # pylint: disable=g-import-not-at-top\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a folder path as an argument with '--log_dir' to save\n",
    "# TensorBoard summaries. Default is a log folder in current directory.\n",
    "# current_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n",
    "current_path = \"/Users/Ganymedian/Desktop/dynamic-rw/output/blog-catalog/m1/\"\n",
    "start_id = 1\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "    '--log_dir',\n",
    "    type=str,\n",
    "    default=os.path.join(current_path, 'log'),\n",
    "    help='The log directory for TensorBoard summaries.')\n",
    "FLAGS, unparsed = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10400\n",
    "batch_size = 100\n",
    "embed_size = 128  # Dimension of the embedding vector.\n",
    "num_sampled = 10  # Number of negative examples to sample.\n",
    "l_rate = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fname):\n",
    "    print(fname)\n",
    "    rws = np.concatenate([\n",
    "        np.loadtxt(f.open(), delimiter='\\t', dtype=int)\n",
    "        for f in rw_location.glob(fname)\n",
    "        if f.stat().st_size > 0\n",
    "    ])\n",
    "    return rws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gPairs-w4-s8.txt\n",
      "Data size 7326400\n"
     ]
    }
   ],
   "source": [
    "rw_location = Path(\"/Users/Ganymedian/Desktop/dynamic-rw/output/blog-catalog/m1/\")\n",
    "filename = \"gPairs-w4-s8.txt\"\n",
    "# filename = \"test.txt\"\n",
    "seed = 1234\n",
    "np.random.seed(seed=seed)\n",
    "data = read_data(filename) - start_id\n",
    "print('Data size', len(data))\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3086 3086]\n",
      " [  65 4025]\n",
      " [5428 5767]\n",
      " ...\n",
      " [7213 7213]\n",
      " [9516 9516]\n",
      " [6250 6250]]\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "data_size = len(data)\n",
    "np.random.seed(seed=seed)\n",
    "np.random.shuffle(data)\n",
    "print(data)\n",
    "batches = np.vsplit(data, data_size/batch_size)\n",
    "# [print(len(m)) for m in batches]\n",
    "\n",
    "def generate_batch():\n",
    "    global data_index\n",
    "    next_batch = batches[data_index]\n",
    "    data_index += 1\n",
    "    targets = next_batch[:,0]\n",
    "    contexts = next_batch[:,1].reshape(len(next_batch),1)\n",
    "    return targets, contexts\n",
    "\n",
    "def end_of_batch():\n",
    "    if (data_index >= len(batches)):\n",
    "        return True \n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def reset_batch_gen():\n",
    "    global data_index\n",
    "    data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. These 3 variables are used only for\n",
    "# displaying model accuracy, they don't affect calculation.\n",
    "valid_size = 2  # Random set of words to evaluate similarity on.\n",
    "valid_window = 10  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  with tf.name_scope('inputs'):\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_contexts = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    with tf.name_scope('embeddings'):\n",
    "        embeddings = tf.Variable(\n",
    "              tf.random_uniform([vocab_size, embed_size], -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    with tf.name_scope('weights'):\n",
    "          nce_weights = tf.Variable(\n",
    "              tf.truncated_normal(\n",
    "                  [vocab_size, embed_size],\n",
    "                  stddev=1.0 / math.sqrt(embed_size)))\n",
    "    with tf.name_scope('biases'):\n",
    "          nce_biases = tf.Variable(tf.zeros([vocab_size]))\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  # Explanation of the meaning of NCE loss:\n",
    "  #   http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.nce_loss(\n",
    "                weights=nce_weights,\n",
    "                biases=nce_biases,\n",
    "                labels=train_contexts,\n",
    "                inputs=embed,\n",
    "                num_sampled=num_sampled,\n",
    "                num_classes=vocab_size))\n",
    "    # Add the loss value as a scalar to summary.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(l_rate).minimize(loss)\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings,\n",
    "                                            valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Merge all summaries.\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Create a saver.\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Begin Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  63.08530807495117\n",
      "Nearest to 2: 7903, 69, 2696, 795, 4127, 1726, 6965, 31,\n",
      "Nearest to 9: 3663, 2677, 7075, 2510, 4442, 4723, 3310, 8678,\n",
      "Average loss at step  50 :  48.314411315917965\n",
      "Average loss at step  100 :  45.79821376800537\n",
      "Average loss at step  150 :  41.96919704437256\n",
      "Average loss at step  200 :  43.060015182495114\n",
      "Average loss at step  250 :  38.82096683502197\n",
      "Average loss at step  300 :  41.89749347686767\n",
      "Average loss at step  350 :  40.47307857513428\n",
      "Average loss at step  400 :  39.24345432281494\n",
      "Average loss at step  450 :  40.01040958404541\n",
      "Average loss at step  500 :  38.97160919189453\n",
      "Average loss at step  550 :  39.45461391448975\n",
      "Average loss at step  600 :  38.2632075881958\n",
      "Average loss at step  650 :  38.364429016113284\n",
      "Average loss at step  700 :  34.37534116744995\n",
      "Average loss at step  750 :  36.2626382446289\n",
      "Average loss at step  800 :  34.55638921737671\n",
      "Average loss at step  850 :  36.31722541809082\n",
      "Average loss at step  900 :  32.170637912750244\n",
      "Average loss at step  950 :  32.883782081604004\n",
      "Average loss at step  1000 :  36.7255633354187\n",
      "Average loss at step  1050 :  36.45600288391113\n",
      "Average loss at step  1100 :  33.434742794036865\n",
      "Average loss at step  1150 :  35.68072189331055\n",
      "Average loss at step  1200 :  36.62476896286011\n",
      "Average loss at step  1250 :  34.63453598022461\n",
      "Average loss at step  1300 :  33.709067020416256\n",
      "Average loss at step  1350 :  32.292654285430906\n",
      "Average loss at step  1400 :  35.876282329559324\n",
      "Average loss at step  1450 :  32.49699317932129\n",
      "Average loss at step  1500 :  33.75814977645874\n",
      "Average loss at step  1550 :  33.51292789459229\n",
      "Average loss at step  1600 :  30.736659450531008\n",
      "Average loss at step  1650 :  30.224398822784423\n",
      "Average loss at step  1700 :  31.038574542999267\n",
      "Average loss at step  1750 :  32.20091171264649\n",
      "Average loss at step  1800 :  29.65513945579529\n",
      "Average loss at step  1850 :  30.786667613983155\n",
      "Average loss at step  1900 :  32.199742279052735\n",
      "Average loss at step  1950 :  30.8408304977417\n",
      "Average loss at step  2000 :  32.470376892089845\n",
      "Average loss at step  2050 :  32.172059326171876\n",
      "Average loss at step  2100 :  31.271703128814696\n",
      "Average loss at step  2150 :  28.11778553009033\n",
      "Average loss at step  2200 :  28.59992328643799\n",
      "Average loss at step  2250 :  30.95814143180847\n",
      "Average loss at step  2300 :  26.487016353607178\n",
      "Average loss at step  2350 :  30.555350818634032\n",
      "Average loss at step  2400 :  30.902862663269044\n",
      "Average loss at step  2450 :  30.45442205429077\n",
      "Average loss at step  2500 :  30.223884897232054\n",
      "Average loss at step  2550 :  29.95908942222595\n",
      "Average loss at step  2600 :  29.388550682067873\n",
      "Average loss at step  2650 :  30.0397013092041\n",
      "Average loss at step  2700 :  30.213094935417175\n",
      "Average loss at step  2750 :  28.815637359619142\n",
      "Average loss at step  2800 :  29.07312321662903\n",
      "Average loss at step  2850 :  28.869558808803557\n",
      "Average loss at step  2900 :  27.61203435897827\n",
      "Average loss at step  2950 :  26.241041641235352\n",
      "Average loss at step  3000 :  27.20662851333618\n",
      "Average loss at step  3050 :  27.40881954193115\n",
      "Average loss at step  3100 :  24.706920363903045\n",
      "Average loss at step  3150 :  27.050697021484375\n",
      "Average loss at step  3200 :  26.484002504348755\n",
      "Average loss at step  3250 :  25.27023208618164\n",
      "Average loss at step  3300 :  24.805657329559327\n",
      "Average loss at step  3350 :  27.241858539581298\n",
      "Average loss at step  3400 :  26.676112594604493\n",
      "Average loss at step  3450 :  27.61306909561157\n",
      "Average loss at step  3500 :  26.311163511276245\n",
      "Average loss at step  3550 :  28.420172452926636\n",
      "Average loss at step  3600 :  26.459981679916382\n",
      "Average loss at step  3650 :  27.30914065361023\n",
      "Average loss at step  3700 :  26.204548873901366\n",
      "Average loss at step  3750 :  25.369296817779542\n",
      "Average loss at step  3800 :  24.70683150112629\n",
      "Average loss at step  3850 :  27.0455914247036\n",
      "Average loss at step  3900 :  24.538787384033203\n",
      "Average loss at step  3950 :  24.951625661849974\n",
      "Average loss at step  4000 :  24.41480266571045\n",
      "Average loss at step  4050 :  26.460342712402344\n",
      "Average loss at step  4100 :  25.331966199874877\n",
      "Average loss at step  4150 :  25.817417869567873\n",
      "Average loss at step  4200 :  23.308688049316405\n",
      "Average loss at step  4250 :  23.459720525741577\n",
      "Average loss at step  4300 :  22.737409508228303\n",
      "Average loss at step  4350 :  23.571085567474366\n",
      "Average loss at step  4400 :  24.65203711986542\n",
      "Average loss at step  4450 :  23.99684455871582\n",
      "Average loss at step  4500 :  23.156500000953674\n",
      "Average loss at step  4550 :  23.911742439270018\n",
      "Average loss at step  4600 :  22.81503451347351\n",
      "Average loss at step  4650 :  22.57821192741394\n",
      "Average loss at step  4700 :  21.676460230350493\n",
      "Average loss at step  4750 :  24.535435814857482\n",
      "Average loss at step  4800 :  23.033394193649293\n",
      "Average loss at step  4850 :  21.206674222946166\n",
      "Average loss at step  4900 :  21.66209825515747\n",
      "Average loss at step  4950 :  22.431643447875977\n",
      "Average loss at step  5000 :  20.75098262786865\n",
      "Average loss at step  5050 :  20.51427001237869\n",
      "Average loss at step  5100 :  20.83155791282654\n",
      "Average loss at step  5150 :  22.173531136512757\n",
      "Average loss at step  5200 :  22.226949417591094\n",
      "Average loss at step  5250 :  20.011486223340036\n",
      "Average loss at step  5300 :  19.500960050821305\n",
      "Average loss at step  5350 :  21.81434036612511\n",
      "Average loss at step  5400 :  23.28296055793762\n",
      "Average loss at step  5450 :  21.690386123657227\n",
      "Average loss at step  5500 :  19.722354230880736\n",
      "Average loss at step  5550 :  18.006908769607545\n",
      "Average loss at step  5600 :  21.469998774528502\n",
      "Average loss at step  5650 :  18.901674785614013\n",
      "Average loss at step  5700 :  20.00791044473648\n",
      "Average loss at step  5750 :  22.31564443230629\n",
      "Average loss at step  5800 :  19.869570598602294\n",
      "Average loss at step  5850 :  17.879189553260805\n",
      "Average loss at step  5900 :  21.764602727890015\n",
      "Average loss at step  5950 :  20.089543571472166\n",
      "Average loss at step  6000 :  18.751229810714722\n",
      "Average loss at step  6050 :  17.266953597068788\n",
      "Average loss at step  6100 :  17.534843089580537\n",
      "Average loss at step  6150 :  17.694705533981324\n",
      "Average loss at step  6200 :  18.022799048423767\n",
      "Average loss at step  6250 :  17.85810199379921\n",
      "Average loss at step  6300 :  16.6098120033741\n",
      "Average loss at step  6350 :  17.358402333259583\n",
      "Average loss at step  6400 :  17.624373898506164\n",
      "Average loss at step  6450 :  19.11630497097969\n",
      "Average loss at step  6500 :  20.127290782928466\n",
      "Average loss at step  6550 :  17.85120454788208\n",
      "Average loss at step  6600 :  16.124899039268495\n",
      "Average loss at step  6650 :  17.34881172657013\n",
      "Average loss at step  6700 :  18.61407217502594\n",
      "Average loss at step  6750 :  15.227433902025222\n",
      "Average loss at step  6800 :  16.87096962183714\n",
      "Average loss at step  6850 :  19.909564488530158\n",
      "Average loss at step  6900 :  18.02778011083603\n",
      "Average loss at step  6950 :  17.384333282113076\n",
      "Average loss at step  7000 :  16.629697127342226\n",
      "Average loss at step  7050 :  16.843493738174438\n",
      "Average loss at step  7100 :  16.744008073806764\n",
      "Average loss at step  7150 :  16.135250471830368\n",
      "Average loss at step  7200 :  17.1435645198822\n",
      "Average loss at step  7250 :  16.667293964624406\n",
      "Average loss at step  7300 :  15.512927043437958\n",
      "Average loss at step  7350 :  16.30298261642456\n",
      "Average loss at step  7400 :  14.730123314857483\n",
      "Average loss at step  7450 :  16.142752960920333\n",
      "Average loss at step  7500 :  18.467553849220277\n",
      "Average loss at step  7550 :  16.3610244846344\n",
      "Average loss at step  7600 :  15.181287543773651\n",
      "Average loss at step  7650 :  15.610160977840424\n",
      "Average loss at step  7700 :  14.533214764595032\n",
      "Average loss at step  7750 :  14.431132974624633\n",
      "Average loss at step  7800 :  14.159722187519073\n",
      "Average loss at step  7850 :  16.669256126880647\n",
      "Average loss at step  7900 :  16.147839131355287\n",
      "Average loss at step  7950 :  14.083024752140044\n",
      "Average loss at step  8000 :  16.282773127555846\n",
      "Average loss at step  8050 :  15.692332398891448\n",
      "Average loss at step  8100 :  14.234657845497132\n",
      "Average loss at step  8150 :  16.252915349006653\n",
      "Average loss at step  8200 :  13.375526103377343\n",
      "Average loss at step  8250 :  15.828591370582581\n",
      "Average loss at step  8300 :  14.691264128684997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  8350 :  15.01264132618904\n",
      "Average loss at step  8400 :  11.382423333525658\n",
      "Average loss at step  8450 :  15.843119797706605\n",
      "Average loss at step  8500 :  13.790881195068359\n",
      "Average loss at step  8550 :  13.214310402870177\n",
      "Average loss at step  8600 :  14.809744005203248\n",
      "Average loss at step  8650 :  12.636033027172088\n",
      "Average loss at step  8700 :  15.619089860916137\n",
      "Average loss at step  8750 :  13.426828072071075\n",
      "Average loss at step  8800 :  12.927043834924698\n",
      "Average loss at step  8850 :  13.510139672756194\n",
      "Average loss at step  8900 :  13.696301810741424\n",
      "Average loss at step  8950 :  14.72129076719284\n",
      "Average loss at step  9000 :  12.750261919498444\n",
      "Average loss at step  9050 :  11.732833297252656\n",
      "Average loss at step  9100 :  13.060602287650108\n",
      "Average loss at step  9150 :  12.036226231455803\n",
      "Average loss at step  9200 :  11.760471987724305\n",
      "Average loss at step  9250 :  13.285990761220456\n",
      "Average loss at step  9300 :  12.263235522508621\n",
      "Average loss at step  9350 :  14.139073813557625\n",
      "Average loss at step  9400 :  11.9828670501709\n",
      "Average loss at step  9450 :  11.986841036081314\n",
      "Average loss at step  9500 :  13.119823925495147\n",
      "Average loss at step  9550 :  12.764180221557616\n",
      "Average loss at step  9600 :  12.647031368017197\n",
      "Average loss at step  9650 :  13.29522444486618\n",
      "Average loss at step  9700 :  11.351115123033523\n",
      "Average loss at step  9750 :  10.020849633812905\n",
      "Average loss at step  9800 :  12.712734602689743\n",
      "Average loss at step  9850 :  10.52006285727024\n",
      "Average loss at step  9900 :  12.792112118005752\n",
      "Average loss at step  9950 :  11.396085999011994\n",
      "Average loss at step  10000 :  10.1442663782835\n",
      "Nearest to 2: 7903, 1205, 3819, 2621, 4838, 4188, 1929, 10296,\n",
      "Nearest to 9: 4983, 6065, 7075, 3560, 7805, 3525, 4996, 5151,\n",
      "Average loss at step  10050 :  12.05624348640442\n",
      "Average loss at step  10100 :  10.437585302591323\n",
      "Average loss at step  10150 :  10.048272038698196\n",
      "Average loss at step  10200 :  11.69973396062851\n",
      "Average loss at step  10250 :  9.804262773990631\n",
      "Average loss at step  10300 :  11.045895894169808\n",
      "Average loss at step  10350 :  11.154863090515137\n",
      "Average loss at step  10400 :  10.207704998850822\n",
      "Average loss at step  10450 :  7.534872807264328\n",
      "Average loss at step  10500 :  10.768858965039254\n",
      "Average loss at step  10550 :  9.991332346200943\n",
      "Average loss at step  10600 :  10.748007595539093\n",
      "Average loss at step  10650 :  10.517891954183579\n",
      "Average loss at step  10700 :  7.945929914712906\n",
      "Average loss at step  10750 :  10.152143281400203\n",
      "Average loss at step  10800 :  11.498041581511497\n",
      "Average loss at step  10850 :  10.250503044128418\n",
      "Average loss at step  10900 :  10.916675144433976\n",
      "Average loss at step  10950 :  9.706679114848376\n",
      "Average loss at step  11000 :  11.406650965213776\n",
      "Average loss at step  11050 :  9.809758870601653\n",
      "Average loss at step  11100 :  8.508809921443463\n",
      "Average loss at step  11150 :  10.024798118472098\n",
      "Average loss at step  11200 :  9.434018499255181\n",
      "Average loss at step  11250 :  10.160107020139694\n",
      "Average loss at step  11300 :  8.057115330100059\n",
      "Average loss at step  11350 :  7.615421786904335\n",
      "Average loss at step  11400 :  8.922009525299073\n",
      "Average loss at step  11450 :  9.643726659417153\n",
      "Average loss at step  11500 :  7.661624262332916\n",
      "Average loss at step  11550 :  9.731439516246319\n",
      "Average loss at step  11600 :  9.690906401872635\n",
      "Average loss at step  11650 :  11.822333422899247\n",
      "Average loss at step  11700 :  9.400961748361588\n",
      "Average loss at step  11750 :  8.304179124832153\n",
      "Average loss at step  11800 :  8.479493104815482\n",
      "Average loss at step  11850 :  8.621212223768234\n",
      "Average loss at step  11900 :  8.567590223550797\n",
      "Average loss at step  11950 :  7.426141666769982\n",
      "Average loss at step  12000 :  8.735093991160392\n",
      "Average loss at step  12050 :  8.726864548325539\n",
      "Average loss at step  12100 :  8.142017966508865\n",
      "Average loss at step  12150 :  8.905221684873105\n",
      "Average loss at step  12200 :  7.997569782733917\n",
      "Average loss at step  12250 :  7.830804104208946\n",
      "Average loss at step  12300 :  7.604034997522831\n",
      "Average loss at step  12350 :  7.536378561854362\n",
      "Average loss at step  12400 :  7.311486281752586\n",
      "Average loss at step  12450 :  8.42247582256794\n",
      "Average loss at step  12500 :  8.163302439749241\n",
      "Average loss at step  12550 :  9.405003889203071\n",
      "Average loss at step  12600 :  6.975703444480896\n",
      "Average loss at step  12650 :  6.793504667282105\n",
      "Average loss at step  12700 :  7.051525549888611\n",
      "Average loss at step  12750 :  6.521489388346672\n",
      "Average loss at step  12800 :  6.308386908173561\n",
      "Average loss at step  12850 :  8.96860935628414\n",
      "Average loss at step  12900 :  9.173085080683231\n",
      "Average loss at step  12950 :  6.8009849286079405\n",
      "Average loss at step  13000 :  8.29270180284977\n",
      "Average loss at step  13050 :  6.803188916444778\n",
      "Average loss at step  13100 :  8.299002388715744\n",
      "Average loss at step  13150 :  6.346830499172211\n",
      "Average loss at step  13200 :  6.175349546074867\n",
      "Average loss at step  13250 :  6.861321360766888\n",
      "Average loss at step  13300 :  9.692723950147629\n",
      "Average loss at step  13350 :  8.085857038497926\n",
      "Average loss at step  13400 :  7.33816882789135\n",
      "Average loss at step  13450 :  7.645865675210953\n",
      "Average loss at step  13500 :  7.074398772716522\n",
      "Average loss at step  13550 :  5.712549024224281\n",
      "Average loss at step  13600 :  5.451911659240722\n",
      "Average loss at step  13650 :  6.588937142491341\n",
      "Average loss at step  13700 :  7.247365600466728\n",
      "Average loss at step  13750 :  5.656393893361091\n",
      "Average loss at step  13800 :  7.400840025842189\n",
      "Average loss at step  13850 :  6.986780430674553\n",
      "Average loss at step  13900 :  6.609695760607719\n",
      "Average loss at step  13950 :  6.892182275056839\n",
      "Average loss at step  14000 :  6.741380345225334\n",
      "Average loss at step  14050 :  7.14115523993969\n",
      "Average loss at step  14100 :  7.107855306863785\n",
      "Average loss at step  14150 :  6.157596031427383\n",
      "Average loss at step  14200 :  4.603052115738392\n",
      "Average loss at step  14250 :  5.959287725090981\n",
      "Average loss at step  14300 :  6.088369810283184\n",
      "Average loss at step  14350 :  6.52773066163063\n",
      "Average loss at step  14400 :  5.780518501400947\n",
      "Average loss at step  14450 :  5.9433639714121815\n",
      "Average loss at step  14500 :  6.165032377541065\n",
      "Average loss at step  14550 :  5.547035413980484\n",
      "Average loss at step  14600 :  6.5885300725698475\n",
      "Average loss at step  14650 :  5.561574990749359\n",
      "Average loss at step  14700 :  4.748361654579639\n",
      "Average loss at step  14750 :  5.421854543089867\n",
      "Average loss at step  14800 :  6.455437225401401\n",
      "Average loss at step  14850 :  4.860429368615151\n",
      "Average loss at step  14900 :  5.321208665966988\n",
      "Average loss at step  14950 :  6.158234474658966\n",
      "Average loss at step  15000 :  4.311879100501537\n",
      "Average loss at step  15050 :  7.433511830568314\n",
      "Average loss at step  15100 :  5.7166499531269075\n",
      "Average loss at step  15150 :  4.7297809356451035\n",
      "Average loss at step  15200 :  5.352132799625397\n",
      "Average loss at step  15250 :  5.229495163857937\n",
      "Average loss at step  15300 :  6.715815725922584\n",
      "Average loss at step  15350 :  6.040905622243881\n",
      "Average loss at step  15400 :  4.355964262485504\n",
      "Average loss at step  15450 :  5.607577279806137\n",
      "Average loss at step  15500 :  5.247984116077423\n",
      "Average loss at step  15550 :  6.359827807545662\n",
      "Average loss at step  15600 :  5.972159028053284\n",
      "Average loss at step  15650 :  5.329190044999122\n",
      "Average loss at step  15700 :  5.1929566764831545\n",
      "Average loss at step  15750 :  5.360036137104035\n",
      "Average loss at step  15800 :  5.580874320268631\n",
      "Average loss at step  15850 :  5.853237338066101\n",
      "Average loss at step  15900 :  4.8971389067173\n",
      "Average loss at step  15950 :  4.500964297652245\n",
      "Average loss at step  16000 :  5.572370392680168\n",
      "Average loss at step  16050 :  5.420649069547653\n",
      "Average loss at step  16100 :  5.250876247882843\n",
      "Average loss at step  16150 :  5.030481460392475\n",
      "Average loss at step  16200 :  4.69924763083458\n",
      "Average loss at step  16250 :  4.5732722848653795\n",
      "Average loss at step  16300 :  5.074972661733628\n",
      "Average loss at step  16350 :  6.488508318066597\n",
      "Average loss at step  16400 :  4.657647432982921\n",
      "Average loss at step  16450 :  5.066006834805012\n",
      "Average loss at step  16500 :  5.167073810100556\n",
      "Average loss at step  16550 :  4.751683250963688\n",
      "Average loss at step  16600 :  4.894920852780342\n",
      "Average loss at step  16650 :  5.16330990344286\n",
      "Average loss at step  16700 :  4.131012591421604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  16750 :  4.464276714622974\n",
      "Average loss at step  16800 :  3.5867829382419587\n",
      "Average loss at step  16850 :  3.0239744439721106\n",
      "Average loss at step  16900 :  5.434337801337242\n",
      "Average loss at step  16950 :  5.01233450114727\n",
      "Average loss at step  17000 :  4.561363677382469\n",
      "Average loss at step  17050 :  5.863080714941025\n",
      "Average loss at step  17100 :  5.225208818912506\n",
      "Average loss at step  17150 :  4.369750583171845\n",
      "Average loss at step  17200 :  3.773611161112785\n",
      "Average loss at step  17250 :  4.718094415664673\n",
      "Average loss at step  17300 :  4.630157750844956\n",
      "Average loss at step  17350 :  5.432171794772148\n",
      "Average loss at step  17400 :  4.003055675029755\n",
      "Average loss at step  17450 :  3.1952126109600067\n",
      "Average loss at step  17500 :  3.190406289100647\n",
      "Average loss at step  17550 :  4.173774354457855\n",
      "Average loss at step  17600 :  4.3415191894769665\n",
      "Average loss at step  17650 :  4.5765102082490925\n",
      "Average loss at step  17700 :  4.0076550242304805\n",
      "Average loss at step  17750 :  4.657577047944069\n",
      "Average loss at step  17800 :  4.340928503274918\n",
      "Average loss at step  17850 :  3.898260909616947\n",
      "Average loss at step  17900 :  3.8503892996907236\n",
      "Average loss at step  17950 :  3.781434801518917\n",
      "Average loss at step  18000 :  3.2431745958328246\n",
      "Average loss at step  18050 :  5.17879287481308\n",
      "Average loss at step  18100 :  4.196026144921779\n",
      "Average loss at step  18150 :  3.973227213025093\n",
      "Average loss at step  18200 :  4.331321598887444\n",
      "Average loss at step  18250 :  3.370104605257511\n",
      "Average loss at step  18300 :  3.651108619272709\n",
      "Average loss at step  18350 :  3.6617955017089843\n",
      "Average loss at step  18400 :  4.092179760634899\n",
      "Average loss at step  18450 :  4.1522231292724605\n",
      "Average loss at step  18500 :  3.6076129168272018\n",
      "Average loss at step  18550 :  4.37953263014555\n",
      "Average loss at step  18600 :  3.4286166608333586\n",
      "Average loss at step  18650 :  3.136025786101818\n",
      "Average loss at step  18700 :  3.6942249405384064\n",
      "Average loss at step  18750 :  4.296907941997051\n",
      "Average loss at step  18800 :  4.033343646526337\n",
      "Average loss at step  18850 :  2.866266630589962\n",
      "Average loss at step  18900 :  3.0682876253128053\n",
      "Average loss at step  18950 :  3.4271431970596313\n",
      "Average loss at step  19000 :  3.682527332305908\n",
      "Average loss at step  19050 :  4.221452648639679\n",
      "Average loss at step  19100 :  3.319516335725784\n",
      "Average loss at step  19150 :  3.0225490200519562\n",
      "Average loss at step  19200 :  4.127437675893307\n",
      "Average loss at step  19250 :  3.288828377425671\n",
      "Average loss at step  19300 :  3.245334413647652\n",
      "Average loss at step  19350 :  4.142885214686394\n",
      "Average loss at step  19400 :  3.793536571264267\n",
      "Average loss at step  19450 :  3.2537630060315132\n",
      "Average loss at step  19500 :  2.971565496623516\n",
      "Average loss at step  19550 :  3.3256909346580503\n",
      "Average loss at step  19600 :  3.4904917418956756\n",
      "Average loss at step  19650 :  2.9975943061709405\n",
      "Average loss at step  19700 :  3.0254926404356954\n",
      "Average loss at step  19750 :  3.1241004240512846\n",
      "Average loss at step  19800 :  3.1605002507567406\n",
      "Average loss at step  19850 :  2.058290359377861\n",
      "Average loss at step  19900 :  3.1349943655729295\n",
      "Average loss at step  19950 :  3.1151447635889054\n",
      "Average loss at step  20000 :  2.140158856511116\n",
      "Nearest to 2: 1205, 3819, 2621, 7903, 10296, 4838, 10244, 5465,\n",
      "Nearest to 9: 4983, 7075, 6065, 7805, 3525, 4707, 9497, 4996,\n",
      "Average loss at step  20050 :  2.5476475065946578\n",
      "Average loss at step  20100 :  3.370782872438431\n",
      "Average loss at step  20150 :  3.295057047307491\n",
      "Average loss at step  20200 :  4.218463568687439\n",
      "Average loss at step  20250 :  2.645804510116577\n",
      "Average loss at step  20300 :  2.8559740018844604\n",
      "Average loss at step  20350 :  2.9369454693794252\n",
      "Average loss at step  20400 :  3.116249847412109\n",
      "Average loss at step  20450 :  3.1908183565735815\n",
      "Average loss at step  20500 :  2.9973571437597273\n",
      "Average loss at step  20550 :  3.0334422454237937\n",
      "Average loss at step  20600 :  1.776638075709343\n",
      "Average loss at step  20650 :  3.4917945861816406\n",
      "Average loss at step  20700 :  2.7151765245199204\n",
      "Average loss at step  20750 :  1.9531993067264557\n",
      "Average loss at step  20800 :  3.1258199006319045\n",
      "Average loss at step  20850 :  3.319870584309101\n",
      "Average loss at step  20900 :  2.4354480892419814\n",
      "Average loss at step  20950 :  2.753537991642952\n",
      "Average loss at step  21000 :  2.7831796151399613\n",
      "Average loss at step  21050 :  2.1730916318297386\n",
      "Average loss at step  21100 :  2.1805138501524923\n",
      "Average loss at step  21150 :  2.870992828309536\n",
      "Average loss at step  21200 :  2.5363059574365616\n",
      "Average loss at step  21250 :  2.864109269678593\n",
      "Average loss at step  21300 :  2.0726704362034796\n",
      "Average loss at step  21350 :  2.327354599237442\n",
      "Average loss at step  21400 :  1.6162863558530807\n",
      "Average loss at step  21450 :  2.015068488717079\n",
      "Average loss at step  21500 :  1.7893157479166986\n",
      "Average loss at step  21550 :  2.702092365026474\n",
      "Average loss at step  21600 :  2.005202552676201\n",
      "Average loss at step  21650 :  2.5687940290570257\n",
      "Average loss at step  21700 :  2.41396756619215\n",
      "Average loss at step  21750 :  2.9874021592736244\n",
      "Average loss at step  21800 :  2.418303127884865\n",
      "Average loss at step  21850 :  2.8910857105255126\n",
      "Average loss at step  21900 :  2.051070044338703\n",
      "Average loss at step  21950 :  2.8595788887143136\n",
      "Average loss at step  22000 :  1.8473633742332458\n",
      "Average loss at step  22050 :  1.8097598177194596\n",
      "Average loss at step  22100 :  2.695088058412075\n",
      "Average loss at step  22150 :  2.6969272762537004\n",
      "Average loss at step  22200 :  2.100104976296425\n",
      "Average loss at step  22250 :  2.2474159970879555\n",
      "Average loss at step  22300 :  1.748701981306076\n",
      "Average loss at step  22350 :  1.9274234527349472\n",
      "Average loss at step  22400 :  2.397731090188026\n",
      "Average loss at step  22450 :  2.262690668106079\n",
      "Average loss at step  22500 :  2.630137915611267\n",
      "Average loss at step  22550 :  2.492638618350029\n",
      "Average loss at step  22600 :  2.1225727051496506\n",
      "Average loss at step  22650 :  2.8321641927957533\n",
      "Average loss at step  22700 :  1.6790496718883514\n",
      "Average loss at step  22750 :  2.0614248743653296\n",
      "Average loss at step  22800 :  2.6658031034469603\n",
      "Average loss at step  22850 :  2.220474956035614\n",
      "Average loss at step  22900 :  1.45029634475708\n",
      "Average loss at step  22950 :  2.1486206874251366\n",
      "Average loss at step  23000 :  2.253119932115078\n",
      "Average loss at step  23050 :  2.163795735538006\n",
      "Average loss at step  23100 :  1.8858340355753898\n",
      "Average loss at step  23150 :  1.4769012200832368\n",
      "Average loss at step  23200 :  2.2007126089930535\n",
      "Average loss at step  23250 :  1.6476463142037392\n",
      "Average loss at step  23300 :  1.817174052298069\n",
      "Average loss at step  23350 :  1.9741245874762534\n",
      "Average loss at step  23400 :  2.2890471389889715\n",
      "Average loss at step  23450 :  2.581039468050003\n",
      "Average loss at step  23500 :  2.4310498693585396\n",
      "Average loss at step  23550 :  1.854453312754631\n",
      "Average loss at step  23600 :  1.5784200644493103\n",
      "Average loss at step  23650 :  2.3669512724876403\n",
      "Average loss at step  23700 :  2.3539951854944228\n",
      "Average loss at step  23750 :  1.955437994003296\n",
      "Average loss at step  23800 :  2.0212183979153635\n",
      "Average loss at step  23850 :  1.5500216588377953\n",
      "Average loss at step  23900 :  2.0639618575572967\n",
      "Average loss at step  23950 :  2.3397532173991205\n",
      "Average loss at step  24000 :  2.3959932386875153\n",
      "Average loss at step  24050 :  2.285293599367142\n",
      "Average loss at step  24100 :  1.843119792342186\n",
      "Average loss at step  24150 :  1.4819650235772133\n",
      "Average loss at step  24200 :  2.5184930616617205\n",
      "Average loss at step  24250 :  1.6742693004012108\n",
      "Average loss at step  24300 :  1.513160594701767\n",
      "Average loss at step  24350 :  1.6140313825011254\n",
      "Average loss at step  24400 :  1.187599948644638\n",
      "Average loss at step  24450 :  1.8592015579342842\n",
      "Average loss at step  24500 :  1.6685378396511077\n",
      "Average loss at step  24550 :  1.9182715573906899\n",
      "Average loss at step  24600 :  1.6550117886066438\n",
      "Average loss at step  24650 :  1.8461908066272736\n",
      "Average loss at step  24700 :  1.4738902941346168\n",
      "Average loss at step  24750 :  1.301187413930893\n",
      "Average loss at step  24800 :  1.6986030316352845\n",
      "Average loss at step  24850 :  1.7736003971099854\n",
      "Average loss at step  24900 :  1.8145847952365874\n",
      "Average loss at step  24950 :  0.9066645562648773\n",
      "Average loss at step  25000 :  1.7914395520091058\n",
      "Average loss at step  25050 :  1.5867598521709443\n",
      "Average loss at step  25100 :  1.8077057707309723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  25150 :  1.8246972700953483\n",
      "Average loss at step  25200 :  1.9799916929006576\n",
      "Average loss at step  25250 :  1.263619665503502\n",
      "Average loss at step  25300 :  1.919578619003296\n",
      "Average loss at step  25350 :  1.9247379148006438\n",
      "Average loss at step  25400 :  1.502674359679222\n",
      "Average loss at step  25450 :  2.116018385589123\n",
      "Average loss at step  25500 :  1.269088074862957\n",
      "Average loss at step  25550 :  2.8276822420954706\n",
      "Average loss at step  25600 :  2.078286197781563\n",
      "Average loss at step  25650 :  2.044270456433296\n",
      "Average loss at step  25700 :  1.8202953836321831\n",
      "Average loss at step  25750 :  1.882304358780384\n",
      "Average loss at step  25800 :  1.7346816742420197\n",
      "Average loss at step  25850 :  1.9458190935850144\n",
      "Average loss at step  25900 :  1.4695910894870758\n",
      "Average loss at step  25950 :  2.0861775487661363\n",
      "Average loss at step  26000 :  2.583795090317726\n",
      "Average loss at step  26050 :  0.9855936422944069\n",
      "Average loss at step  26100 :  1.9169980916380882\n",
      "Average loss at step  26150 :  1.5507468670606612\n",
      "Average loss at step  26200 :  1.9078872266411782\n",
      "Average loss at step  26250 :  1.2804311460256577\n",
      "Average loss at step  26300 :  1.2215559402108191\n",
      "Average loss at step  26350 :  1.3808417177200318\n",
      "Average loss at step  26400 :  1.6161250805854797\n",
      "Average loss at step  26450 :  1.4087988623976708\n",
      "Average loss at step  26500 :  1.3355127310752868\n",
      "Average loss at step  26550 :  2.2317805695533752\n",
      "Average loss at step  26600 :  1.5732621043920516\n",
      "Average loss at step  26650 :  1.4577127280831337\n",
      "Average loss at step  26700 :  1.348565909564495\n",
      "Average loss at step  26750 :  1.4281854727864265\n",
      "Average loss at step  26800 :  1.2393280673027038\n",
      "Average loss at step  26850 :  1.50852449208498\n",
      "Average loss at step  26900 :  1.179310061633587\n",
      "Average loss at step  26950 :  0.986036805510521\n",
      "Average loss at step  27000 :  1.3774787181615828\n",
      "Average loss at step  27050 :  1.4990982222557068\n",
      "Average loss at step  27100 :  1.1933963698148728\n",
      "Average loss at step  27150 :  1.240939029455185\n",
      "Average loss at step  27200 :  1.1994256469607354\n",
      "Average loss at step  27250 :  1.9233134296536445\n",
      "Average loss at step  27300 :  0.7667575815320015\n",
      "Average loss at step  27350 :  1.264611787199974\n",
      "Average loss at step  27400 :  1.0135482153296471\n",
      "Average loss at step  27450 :  1.1885505908727645\n",
      "Average loss at step  27500 :  1.4672944650053978\n",
      "Average loss at step  27550 :  1.4629521530866623\n",
      "Average loss at step  27600 :  1.256071979701519\n",
      "Average loss at step  27650 :  1.5644710460305213\n",
      "Average loss at step  27700 :  1.2183155032992363\n",
      "Average loss at step  27750 :  1.1970327277481556\n",
      "Average loss at step  27800 :  1.253456497490406\n",
      "Average loss at step  27850 :  1.0820564225316047\n",
      "Average loss at step  27900 :  1.1399823784828187\n",
      "Average loss at step  27950 :  1.4370838302373885\n",
      "Average loss at step  28000 :  1.931640678346157\n",
      "Average loss at step  28050 :  0.9891372691094875\n",
      "Average loss at step  28100 :  1.5413665613532066\n",
      "Average loss at step  28150 :  0.8131952205300331\n",
      "Average loss at step  28200 :  1.3111725494265556\n",
      "Average loss at step  28250 :  1.1202350860834123\n",
      "Average loss at step  28300 :  1.065578096807003\n",
      "Average loss at step  28350 :  1.3152785390615462\n",
      "Average loss at step  28400 :  1.5326576244831085\n",
      "Average loss at step  28450 :  0.9369506293535232\n",
      "Average loss at step  28500 :  0.9793602535128594\n",
      "Average loss at step  28550 :  1.651997049152851\n",
      "Average loss at step  28600 :  1.0538367538154125\n",
      "Average loss at step  28650 :  1.539636367559433\n",
      "Average loss at step  28700 :  1.659418765604496\n",
      "Average loss at step  28750 :  0.7722603127360343\n",
      "Average loss at step  28800 :  1.1048239916563034\n",
      "Average loss at step  28850 :  1.2481359261274338\n",
      "Average loss at step  28900 :  1.264037892818451\n",
      "Average loss at step  28950 :  1.0930705165863037\n",
      "Average loss at step  29000 :  1.2576591646671296\n",
      "Average loss at step  29050 :  1.431753759086132\n",
      "Average loss at step  29100 :  0.9387374174594879\n",
      "Average loss at step  29150 :  1.1150129380822182\n",
      "Average loss at step  29200 :  1.5901935216784477\n",
      "Average loss at step  29250 :  0.896639309823513\n",
      "Average loss at step  29300 :  1.3647875955700874\n",
      "Average loss at step  29350 :  1.1507932737469673\n",
      "Average loss at step  29400 :  1.373502488732338\n",
      "Average loss at step  29450 :  1.5065700706839562\n",
      "Average loss at step  29500 :  1.3309536826610566\n",
      "Average loss at step  29550 :  0.911834003329277\n",
      "Average loss at step  29600 :  1.1498795291781425\n",
      "Average loss at step  29650 :  0.8968825849890709\n",
      "Average loss at step  29700 :  1.0699873530864716\n",
      "Average loss at step  29750 :  1.0400533896684647\n",
      "Average loss at step  29800 :  1.189687272310257\n",
      "Average loss at step  29850 :  1.3647088542580605\n",
      "Average loss at step  29900 :  0.8624762314558029\n",
      "Average loss at step  29950 :  0.8158225303888321\n",
      "Average loss at step  30000 :  0.9734136137366295\n",
      "Nearest to 2: 3819, 1205, 7903, 2621, 10244, 10296, 3763, 6182,\n",
      "Nearest to 9: 7075, 6065, 4983, 9497, 2677, 864, 4068, 6279,\n",
      "Average loss at step  30050 :  0.8984830614924431\n",
      "Average loss at step  30100 :  0.898331304192543\n",
      "Average loss at step  30150 :  1.1168477365374565\n",
      "Average loss at step  30200 :  0.9782692512869835\n",
      "Average loss at step  30250 :  0.9663077473640442\n",
      "Average loss at step  30300 :  1.222646504342556\n",
      "Average loss at step  30350 :  0.8471532911062241\n",
      "Average loss at step  30400 :  1.081057938337326\n",
      "Average loss at step  30450 :  1.2247080302238464\n",
      "Average loss at step  30500 :  1.531431176364422\n",
      "Average loss at step  30550 :  0.93947355479002\n",
      "Average loss at step  30600 :  1.1065228706598282\n",
      "Average loss at step  30650 :  1.4671322366595267\n",
      "Average loss at step  30700 :  1.2510978969931603\n",
      "Average loss at step  30750 :  1.4488611820340156\n",
      "Average loss at step  30800 :  1.1580974489450455\n",
      "Average loss at step  30850 :  1.5029323345422745\n",
      "Average loss at step  30900 :  0.7382691125571728\n",
      "Average loss at step  30950 :  1.3387033966183663\n",
      "Average loss at step  31000 :  1.0554112154245376\n",
      "Average loss at step  31050 :  1.0029087975621223\n",
      "Average loss at step  31100 :  0.7252620363235474\n",
      "Average loss at step  31150 :  0.7340492483973503\n",
      "Average loss at step  31200 :  1.2572992151975633\n",
      "Average loss at step  31250 :  0.7755907440185547\n",
      "Average loss at step  31300 :  1.0160710829496384\n",
      "Average loss at step  31350 :  0.768297489285469\n",
      "Average loss at step  31400 :  0.4787939715385437\n",
      "Average loss at step  31450 :  0.9289883080124856\n",
      "Average loss at step  31500 :  0.7803942802548408\n",
      "Average loss at step  31550 :  0.7460262498259544\n",
      "Average loss at step  31600 :  0.6968596211075783\n",
      "Average loss at step  31650 :  1.0302763307094573\n",
      "Average loss at step  31700 :  1.1006484478712082\n",
      "Average loss at step  31750 :  0.9497549757361412\n",
      "Average loss at step  31800 :  0.801877493262291\n",
      "Average loss at step  31850 :  1.1776665207743644\n",
      "Average loss at step  31900 :  1.1065151670575142\n",
      "Average loss at step  31950 :  0.6056882053613662\n",
      "Average loss at step  32000 :  0.7620807141065598\n",
      "Average loss at step  32050 :  1.1586865797638892\n",
      "Average loss at step  32100 :  1.1583799463510513\n",
      "Average loss at step  32150 :  0.6906592255830765\n",
      "Average loss at step  32200 :  1.3208601826429367\n",
      "Average loss at step  32250 :  0.7300941698253155\n",
      "Average loss at step  32300 :  1.2096717946231366\n",
      "Average loss at step  32350 :  0.6645549830794334\n",
      "Average loss at step  32400 :  0.8181354367733001\n",
      "Average loss at step  32450 :  1.005024939775467\n",
      "Average loss at step  32500 :  0.6793010395765304\n",
      "Average loss at step  32550 :  0.7970984470844269\n",
      "Average loss at step  32600 :  0.7901739919185639\n",
      "Average loss at step  32650 :  1.1018117287755012\n",
      "Average loss at step  32700 :  0.8661389517784118\n",
      "Average loss at step  32750 :  0.7237949198484421\n",
      "Average loss at step  32800 :  1.2607704696059228\n",
      "Average loss at step  32850 :  0.8611124745011329\n",
      "Average loss at step  32900 :  0.5146094675362111\n",
      "Average loss at step  32950 :  0.7312932777404785\n",
      "Average loss at step  33000 :  0.7003860902786255\n",
      "Average loss at step  33050 :  1.2025198295712471\n",
      "Average loss at step  33100 :  1.3044734010100365\n",
      "Average loss at step  33150 :  0.9873598748445511\n",
      "Average loss at step  33200 :  1.405751491189003\n",
      "Average loss at step  33250 :  0.9533479049801826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  33300 :  0.8602476555109024\n",
      "Average loss at step  33350 :  1.1544659945368767\n",
      "Average loss at step  33400 :  0.7189134985208512\n",
      "Average loss at step  33450 :  0.7861710864305497\n",
      "Average loss at step  33500 :  0.4567035087943077\n",
      "Average loss at step  33550 :  1.3409055218100547\n",
      "Average loss at step  33600 :  0.6656280435621739\n",
      "Average loss at step  33650 :  0.622572869360447\n",
      "Average loss at step  33700 :  1.1125514990091323\n",
      "Average loss at step  33750 :  0.8135216936469079\n",
      "Average loss at step  33800 :  0.965311570763588\n",
      "Average loss at step  33850 :  0.5324739542603493\n",
      "Average loss at step  33900 :  0.9431503358483314\n",
      "Average loss at step  33950 :  1.0781766164302826\n",
      "Average loss at step  34000 :  1.137687792479992\n",
      "Average loss at step  34050 :  0.7395326448976993\n",
      "Average loss at step  34100 :  0.8143764840066433\n",
      "Average loss at step  34150 :  1.388533134162426\n",
      "Average loss at step  34200 :  0.6112580372393132\n",
      "Average loss at step  34250 :  0.7569889733195305\n",
      "Average loss at step  34300 :  0.6899085840582848\n",
      "Average loss at step  34350 :  0.8784512755274773\n",
      "Average loss at step  34400 :  0.7113038513064385\n",
      "Average loss at step  34450 :  0.6845615364611148\n",
      "Average loss at step  34500 :  1.192018083333969\n",
      "Average loss at step  34550 :  0.6903088101744652\n",
      "Average loss at step  34600 :  0.7267498046159744\n",
      "Average loss at step  34650 :  0.6108684718608857\n",
      "Average loss at step  34700 :  0.8807414609193802\n",
      "Average loss at step  34750 :  0.7817543044686317\n",
      "Average loss at step  34800 :  0.6902286434173583\n",
      "Average loss at step  34850 :  0.91254100933671\n",
      "Average loss at step  34900 :  0.7454320630431175\n",
      "Average loss at step  34950 :  0.5543105094134808\n",
      "Average loss at step  35000 :  0.8182018277049065\n",
      "Average loss at step  35050 :  0.9886857607960701\n",
      "Average loss at step  35100 :  0.6854497824609279\n",
      "Average loss at step  35150 :  0.9012330856919288\n",
      "Average loss at step  35200 :  0.9426955816149711\n",
      "Average loss at step  35250 :  0.4117897851765156\n",
      "Average loss at step  35300 :  0.713266813158989\n",
      "Average loss at step  35350 :  0.753766118735075\n",
      "Average loss at step  35400 :  1.0917903614044189\n",
      "Average loss at step  35450 :  0.6941568353772163\n",
      "Average loss at step  35500 :  0.6946490690112114\n",
      "Average loss at step  35550 :  0.6724566614627838\n",
      "Average loss at step  35600 :  0.5445597869157791\n",
      "Average loss at step  35650 :  0.49738476634025575\n",
      "Average loss at step  35700 :  0.9351632928848267\n",
      "Average loss at step  35750 :  0.6794477827847004\n",
      "Average loss at step  35800 :  0.6233981843292713\n",
      "Average loss at step  35850 :  0.7813052970170975\n",
      "Average loss at step  35900 :  1.143384734094143\n",
      "Average loss at step  35950 :  0.7272852893173695\n",
      "Average loss at step  36000 :  0.8502943906188011\n",
      "Average loss at step  36050 :  0.503775840550661\n",
      "Average loss at step  36100 :  0.4135110151767731\n",
      "Average loss at step  36150 :  0.8034661057591438\n",
      "Average loss at step  36200 :  0.6449005591869355\n",
      "Average loss at step  36250 :  0.7199034735560417\n",
      "Average loss at step  36300 :  1.003723947405815\n",
      "Average loss at step  36350 :  0.7127154499292374\n",
      "Average loss at step  36400 :  0.4464536328613758\n",
      "Average loss at step  36450 :  0.7304149168729782\n",
      "Average loss at step  36500 :  0.533918068408966\n",
      "Average loss at step  36550 :  0.6823630046844482\n",
      "Average loss at step  36600 :  0.6458752757310867\n",
      "Average loss at step  36650 :  0.9096377289295197\n",
      "Average loss at step  36700 :  0.8593878588080406\n",
      "Average loss at step  36750 :  0.5889708811044693\n",
      "Average loss at step  36800 :  0.6163220816850662\n",
      "Average loss at step  36850 :  0.4550996346771717\n",
      "Average loss at step  36900 :  0.6161145529150963\n",
      "Average loss at step  36950 :  0.5467950189113617\n",
      "Average loss at step  37000 :  0.5614316311478614\n",
      "Average loss at step  37050 :  0.8174808257818222\n",
      "Average loss at step  37100 :  0.7653315931558609\n",
      "Average loss at step  37150 :  0.976570762693882\n",
      "Average loss at step  37200 :  0.6328817403316498\n",
      "Average loss at step  37250 :  0.6668257595598698\n",
      "Average loss at step  37300 :  0.7121261464059353\n",
      "Average loss at step  37350 :  0.5985009208321571\n",
      "Average loss at step  37400 :  0.7127446156740188\n",
      "Average loss at step  37450 :  0.5446111115813256\n",
      "Average loss at step  37500 :  0.4781144732236862\n",
      "Average loss at step  37550 :  0.5301963323354721\n",
      "Average loss at step  37600 :  0.41551932141184805\n",
      "Average loss at step  37650 :  0.4118600851297379\n",
      "Average loss at step  37700 :  0.4799117675423622\n",
      "Average loss at step  37750 :  0.7153460831940174\n",
      "Average loss at step  37800 :  0.6192518270015717\n",
      "Average loss at step  37850 :  0.4026771166920662\n",
      "Average loss at step  37900 :  0.44145154893398286\n",
      "Average loss at step  37950 :  0.4771786057949066\n",
      "Average loss at step  38000 :  0.4558701656758785\n",
      "Average loss at step  38050 :  0.4538617154955864\n",
      "Average loss at step  38100 :  0.6456205523014069\n",
      "Average loss at step  38150 :  0.7493163228034974\n",
      "Average loss at step  38200 :  0.47557152569293976\n",
      "Average loss at step  38250 :  0.521505581587553\n",
      "Average loss at step  38300 :  0.5677990332245827\n",
      "Average loss at step  38350 :  0.5431212240457535\n",
      "Average loss at step  38400 :  0.5871124565601349\n",
      "Average loss at step  38450 :  0.8146527722477913\n",
      "Average loss at step  38500 :  0.4887252131104469\n",
      "Average loss at step  38550 :  0.5509098306298256\n",
      "Average loss at step  38600 :  0.407223631888628\n",
      "Average loss at step  38650 :  0.5173498630523682\n",
      "Average loss at step  38700 :  0.36858528316020966\n",
      "Average loss at step  38750 :  0.5001684461534023\n",
      "Average loss at step  38800 :  0.6275147384405136\n",
      "Average loss at step  38850 :  0.6233043059706688\n",
      "Average loss at step  38900 :  0.4854755590856075\n",
      "Average loss at step  38950 :  0.4888713517785072\n",
      "Average loss at step  39000 :  0.6489849561452865\n",
      "Average loss at step  39050 :  0.6523204746842385\n",
      "Average loss at step  39100 :  0.6572539865970611\n",
      "Average loss at step  39150 :  0.530037512332201\n",
      "Average loss at step  39200 :  0.8690846468508243\n",
      "Average loss at step  39250 :  0.5078349447250367\n",
      "Average loss at step  39300 :  0.6999553915858269\n",
      "Average loss at step  39350 :  0.5162889337539673\n",
      "Average loss at step  39400 :  0.4807918344438076\n",
      "Average loss at step  39450 :  0.6997309236228466\n",
      "Average loss at step  39500 :  0.38216533824801446\n",
      "Average loss at step  39550 :  0.6770682004094124\n",
      "Average loss at step  39600 :  0.6861290955543518\n",
      "Average loss at step  39650 :  0.7335131394863129\n",
      "Average loss at step  39700 :  0.6067769715189933\n",
      "Average loss at step  39750 :  0.3972716121375561\n",
      "Average loss at step  39800 :  0.5618362401425838\n",
      "Average loss at step  39850 :  0.5899964094161987\n",
      "Average loss at step  39900 :  0.6211935836076736\n",
      "Average loss at step  39950 :  0.6058613461256027\n",
      "Average loss at step  40000 :  0.5868491604924202\n",
      "Nearest to 2: 1205, 7903, 3819, 10244, 3763, 6182, 2621, 766,\n",
      "Nearest to 9: 7075, 6065, 9497, 2677, 6279, 4983, 9060, 4068,\n",
      "Average loss at step  40050 :  0.5363907966017724\n",
      "Average loss at step  40100 :  0.7665268898010253\n",
      "Average loss at step  40150 :  0.4977347718179226\n",
      "Average loss at step  40200 :  0.573846775740385\n",
      "Average loss at step  40250 :  0.5027910658717155\n",
      "Average loss at step  40300 :  0.5163477732241154\n",
      "Average loss at step  40350 :  0.7086060345172882\n",
      "Average loss at step  40400 :  0.8275062197446823\n",
      "Average loss at step  40450 :  0.4234997998178005\n",
      "Average loss at step  40500 :  0.5018776771426201\n",
      "Average loss at step  40550 :  0.43915176272392276\n",
      "Average loss at step  40600 :  0.5170555190742016\n",
      "Average loss at step  40650 :  0.42790093943476676\n",
      "Average loss at step  40700 :  0.44618416503071784\n",
      "Average loss at step  40750 :  0.7553685528039932\n",
      "Average loss at step  40800 :  0.30552553027868273\n",
      "Average loss at step  40850 :  0.35828821241855624\n",
      "Average loss at step  40900 :  0.35740908712148667\n",
      "Average loss at step  40950 :  0.44465444818139077\n",
      "Average loss at step  41000 :  0.6869109256565571\n",
      "Average loss at step  41050 :  0.47155277878046037\n",
      "Average loss at step  41100 :  0.5691549035906792\n",
      "Average loss at step  41150 :  0.6116265413165093\n",
      "Average loss at step  41200 :  0.6329874257743359\n",
      "Average loss at step  41250 :  0.5044275891780853\n",
      "Average loss at step  41300 :  0.6604967267811298\n",
      "Average loss at step  41350 :  0.5743824675679207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  41400 :  0.46856444656848906\n",
      "Average loss at step  41450 :  0.4989230717718601\n",
      "Average loss at step  41500 :  0.5051733760535717\n",
      "Average loss at step  41550 :  0.5066236099600792\n",
      "Average loss at step  41600 :  0.5999824091792106\n",
      "Average loss at step  41650 :  0.3798510381579399\n",
      "Average loss at step  41700 :  0.6037141571938992\n",
      "Average loss at step  41750 :  0.4321805380284786\n",
      "Average loss at step  41800 :  0.5381005866825581\n",
      "Average loss at step  41850 :  0.4866219379007816\n",
      "Average loss at step  41900 :  0.523280523121357\n",
      "Average loss at step  41950 :  0.4229298160970211\n",
      "Average loss at step  42000 :  0.4153603364527225\n",
      "Average loss at step  42050 :  0.3772012674808502\n",
      "Average loss at step  42100 :  0.41178472846746444\n",
      "Average loss at step  42150 :  0.3927138581871986\n",
      "Average loss at step  42200 :  0.38846427232027053\n",
      "Average loss at step  42250 :  0.6088311514258384\n",
      "Average loss at step  42300 :  0.5240759889781476\n",
      "Average loss at step  42350 :  0.3855301403999329\n",
      "Average loss at step  42400 :  0.38222685903310777\n",
      "Average loss at step  42450 :  0.42577162191271783\n",
      "Average loss at step  42500 :  0.43980589479207993\n",
      "Average loss at step  42550 :  0.6413080663979054\n",
      "Average loss at step  42600 :  0.4978297266364098\n",
      "Average loss at step  42650 :  0.7223874844610692\n",
      "Average loss at step  42700 :  0.8326414097845555\n",
      "Average loss at step  42750 :  0.4517392003536224\n",
      "Average loss at step  42800 :  0.3813577425479889\n",
      "Average loss at step  42850 :  0.5539868727326394\n",
      "Average loss at step  42900 :  0.6461261557042599\n",
      "Average loss at step  42950 :  0.3670338563621044\n",
      "Average loss at step  43000 :  0.40135443687438965\n",
      "Average loss at step  43050 :  0.6758227983117103\n",
      "Average loss at step  43100 :  0.34032191276550294\n",
      "Average loss at step  43150 :  0.318827868103981\n",
      "Average loss at step  43200 :  0.3000720299780369\n",
      "Average loss at step  43250 :  0.6050294630229474\n",
      "Average loss at step  43300 :  0.3327204357087612\n",
      "Average loss at step  43350 :  0.5590603683888912\n",
      "Average loss at step  43400 :  0.4038933800160885\n",
      "Average loss at step  43450 :  0.7475574667751789\n",
      "Average loss at step  43500 :  0.3723504382371903\n",
      "Average loss at step  43550 :  0.4694108226895332\n",
      "Average loss at step  43600 :  0.5979977217316628\n",
      "Average loss at step  43650 :  0.4540409845113754\n",
      "Average loss at step  43700 :  0.4535104106366634\n",
      "Average loss at step  43750 :  0.39196383848786354\n",
      "Average loss at step  43800 :  0.6301761616766453\n",
      "Average loss at step  43850 :  0.3999016004055738\n",
      "Average loss at step  43900 :  0.632237805724144\n",
      "Average loss at step  43950 :  0.5568525359034538\n",
      "Average loss at step  44000 :  0.574879666864872\n",
      "Average loss at step  44050 :  0.37525565072894096\n",
      "Average loss at step  44100 :  0.6108636343479157\n",
      "Average loss at step  44150 :  0.4180221277475357\n",
      "Average loss at step  44200 :  0.4817793324589729\n",
      "Average loss at step  44250 :  0.46680229291319847\n",
      "Average loss at step  44300 :  0.4595602312684059\n",
      "Average loss at step  44350 :  0.4694345574080944\n",
      "Average loss at step  44400 :  0.42302809447050094\n",
      "Average loss at step  44450 :  0.5114059990644455\n",
      "Average loss at step  44500 :  0.40759898886084556\n",
      "Average loss at step  44550 :  0.599580394178629\n",
      "Average loss at step  44600 :  0.3757804349064827\n",
      "Average loss at step  44650 :  0.7269485065340996\n",
      "Average loss at step  44700 :  0.2892669881880283\n",
      "Average loss at step  44750 :  0.5621436090767383\n",
      "Average loss at step  44800 :  0.41011073991656305\n",
      "Average loss at step  44850 :  0.5192122310400009\n",
      "Average loss at step  44900 :  0.4876833918690682\n",
      "Average loss at step  44950 :  0.5799268324673176\n",
      "Average loss at step  45000 :  0.28067940548062326\n",
      "Average loss at step  45050 :  0.33677812933921814\n",
      "Average loss at step  45100 :  0.39072631239891054\n",
      "Average loss at step  45150 :  0.7270477059483528\n",
      "Average loss at step  45200 :  0.3105365699529648\n",
      "Average loss at step  45250 :  0.3837532278895378\n",
      "Average loss at step  45300 :  0.34494392842054367\n",
      "Average loss at step  45350 :  0.49963197246193886\n",
      "Average loss at step  45400 :  0.6019317269325256\n",
      "Average loss at step  45450 :  0.511541519612074\n",
      "Average loss at step  45500 :  0.4786328446865082\n",
      "Average loss at step  45550 :  0.3475197599828243\n",
      "Average loss at step  45600 :  0.37121916085481643\n",
      "Average loss at step  45650 :  0.27987478688359263\n",
      "Average loss at step  45700 :  0.30452010288834575\n",
      "Average loss at step  45750 :  0.3306777125597\n",
      "Average loss at step  45800 :  0.3045811402797699\n",
      "Average loss at step  45850 :  0.3286032220721245\n",
      "Average loss at step  45900 :  0.6907022431492805\n",
      "Average loss at step  45950 :  0.5548735232651234\n",
      "Average loss at step  46000 :  0.36960800051689147\n",
      "Average loss at step  46050 :  0.3847590333223343\n",
      "Average loss at step  46100 :  0.41047653421759606\n",
      "Average loss at step  46150 :  0.2711632467806339\n",
      "Average loss at step  46200 :  0.4067532832920551\n",
      "Average loss at step  46250 :  0.30926736176013947\n",
      "Average loss at step  46300 :  0.526530156582594\n",
      "Average loss at step  46350 :  0.37211630880832675\n",
      "Average loss at step  46400 :  0.40507896602153776\n",
      "Average loss at step  46450 :  0.42086228147149085\n",
      "Average loss at step  46500 :  0.3645109838247299\n",
      "Average loss at step  46550 :  0.3238260091841221\n",
      "Average loss at step  46600 :  0.33439681336283683\n",
      "Average loss at step  46650 :  0.40249643221497533\n",
      "Average loss at step  46700 :  0.33708352282643317\n",
      "Average loss at step  46750 :  0.5144256074726582\n",
      "Average loss at step  46800 :  0.27205355107784274\n",
      "Average loss at step  46850 :  0.43404190719127655\n",
      "Average loss at step  46900 :  0.24695678904652596\n",
      "Average loss at step  46950 :  0.3035125873982906\n",
      "Average loss at step  47000 :  0.45719402343034743\n",
      "Average loss at step  47050 :  0.5954322722554207\n",
      "Average loss at step  47100 :  0.38678503051400187\n",
      "Average loss at step  47150 :  0.3729577212035656\n",
      "Average loss at step  47200 :  0.3821093884110451\n",
      "Average loss at step  47250 :  0.5919284048676491\n",
      "Average loss at step  47300 :  0.3558015109598637\n",
      "Average loss at step  47350 :  0.3870761521160603\n",
      "Average loss at step  47400 :  0.6325718146562577\n",
      "Average loss at step  47450 :  0.33690581932663916\n",
      "Average loss at step  47500 :  0.3816508197784424\n",
      "Average loss at step  47550 :  0.4329041090607643\n",
      "Average loss at step  47600 :  0.3201613949239254\n",
      "Average loss at step  47650 :  0.5183589917421341\n",
      "Average loss at step  47700 :  0.7301430401206016\n",
      "Average loss at step  47750 :  0.5247915796935558\n",
      "Average loss at step  47800 :  0.29543061152100564\n",
      "Average loss at step  47850 :  0.3376782049238682\n",
      "Average loss at step  47900 :  0.43523603558540347\n",
      "Average loss at step  47950 :  0.33310573533177373\n",
      "Average loss at step  48000 :  0.28944619223475454\n",
      "Average loss at step  48050 :  0.30356246784329416\n",
      "Average loss at step  48100 :  0.30847607329487803\n",
      "Average loss at step  48150 :  0.32103981301188467\n",
      "Average loss at step  48200 :  0.5010649605095386\n",
      "Average loss at step  48250 :  0.599376687258482\n",
      "Average loss at step  48300 :  0.4079725231230259\n",
      "Average loss at step  48350 :  0.5739231905341149\n",
      "Average loss at step  48400 :  0.36460497364401817\n",
      "Average loss at step  48450 :  0.3775995288789272\n",
      "Average loss at step  48500 :  0.27533124923706054\n",
      "Average loss at step  48550 :  0.3040831780433655\n",
      "Average loss at step  48600 :  0.3059297725558281\n",
      "Average loss at step  48650 :  0.28306777149438855\n",
      "Average loss at step  48700 :  0.30073410585522653\n",
      "Average loss at step  48750 :  0.394119226783514\n",
      "Average loss at step  48800 :  0.4302958352863789\n",
      "Average loss at step  48850 :  0.29311798959970475\n",
      "Average loss at step  48900 :  0.3238866539299488\n",
      "Average loss at step  48950 :  0.2489211829006672\n",
      "Average loss at step  49000 :  0.3343203896284103\n",
      "Average loss at step  49050 :  0.36018671706318856\n",
      "Average loss at step  49100 :  0.48657951965928076\n",
      "Average loss at step  49150 :  0.2897771915793419\n",
      "Average loss at step  49200 :  0.4142963376641273\n",
      "Average loss at step  49250 :  0.41356154903769493\n",
      "Average loss at step  49300 :  0.42703156396746633\n",
      "Average loss at step  49350 :  0.6735813269019126\n",
      "Average loss at step  49400 :  0.30274235546588896\n",
      "Average loss at step  49450 :  0.5686371864378452\n",
      "Average loss at step  49500 :  0.36070328310132027\n",
      "Average loss at step  49550 :  0.29501497745513916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  49600 :  0.3216203212738037\n",
      "Average loss at step  49650 :  0.4039351487159729\n",
      "Average loss at step  49700 :  0.22380666732788085\n",
      "Average loss at step  49750 :  0.4499937263131142\n",
      "Average loss at step  49800 :  0.4317432673275471\n",
      "Average loss at step  49850 :  0.3141426561772823\n",
      "Average loss at step  49900 :  0.3373208041489124\n",
      "Average loss at step  49950 :  0.30662885785102845\n",
      "Average loss at step  50000 :  0.3126246640086174\n",
      "Nearest to 2: 1205, 7903, 3819, 10244, 3763, 6182, 766, 2061,\n",
      "Nearest to 9: 7075, 9497, 6065, 2677, 6279, 2976, 1345, 9060,\n",
      "Average loss at step  50050 :  0.24156928837299346\n",
      "Average loss at step  50100 :  0.3334947000443935\n",
      "Average loss at step  50150 :  0.380252515822649\n",
      "Average loss at step  50200 :  0.39340526446700097\n",
      "Average loss at step  50250 :  0.3056362071633339\n",
      "Average loss at step  50300 :  0.2595731093734503\n",
      "Average loss at step  50350 :  0.48915335059165954\n",
      "Average loss at step  50400 :  0.34403065644204617\n",
      "Average loss at step  50450 :  0.4283768227696419\n",
      "Average loss at step  50500 :  0.34887825399637223\n",
      "Average loss at step  50550 :  0.2864980602264404\n",
      "Average loss at step  50600 :  0.3867859449982643\n",
      "Average loss at step  50650 :  0.45788314923644063\n",
      "Average loss at step  50700 :  0.38112102419137955\n",
      "Average loss at step  50750 :  0.3339384464919567\n",
      "Average loss at step  50800 :  0.40549959912896155\n",
      "Average loss at step  50850 :  0.29557303309440613\n",
      "Average loss at step  50900 :  0.317595329284668\n",
      "Average loss at step  50950 :  0.26210482910275457\n",
      "Average loss at step  51000 :  0.41500528693199157\n",
      "Average loss at step  51050 :  0.31361251264810563\n",
      "Average loss at step  51100 :  0.28720047786831854\n",
      "Average loss at step  51150 :  0.2728987219929695\n",
      "Average loss at step  51200 :  0.46557234779000284\n",
      "Average loss at step  51250 :  0.25517911061644555\n",
      "Average loss at step  51300 :  0.3364079649746418\n",
      "Average loss at step  51350 :  0.3735055151581764\n",
      "Average loss at step  51400 :  0.29063938319683075\n",
      "Average loss at step  51450 :  0.3557761970162392\n",
      "Average loss at step  51500 :  0.2946309646964073\n",
      "Average loss at step  51550 :  0.2528978604078293\n",
      "Average loss at step  51600 :  0.23621395751833915\n",
      "Average loss at step  51650 :  0.27584192976355554\n",
      "Average loss at step  51700 :  0.2694668170809746\n",
      "Average loss at step  51750 :  0.35805464133620263\n",
      "Average loss at step  51800 :  0.2887240585684776\n",
      "Average loss at step  51850 :  0.3956620764732361\n",
      "Average loss at step  51900 :  0.2697853906452656\n",
      "Average loss at step  51950 :  0.3188335819542408\n",
      "Average loss at step  52000 :  0.31506334453821183\n",
      "Average loss at step  52050 :  0.29408399999141693\n",
      "Average loss at step  52100 :  0.42668001174926756\n",
      "Average loss at step  52150 :  0.3953506222367287\n",
      "Average loss at step  52200 :  0.30139847204089165\n",
      "Average loss at step  52250 :  0.4010081779956818\n",
      "Average loss at step  52300 :  0.29901944264769553\n",
      "Average loss at step  52350 :  0.34836146503686904\n",
      "Average loss at step  52400 :  0.2928155255317688\n",
      "Average loss at step  52450 :  0.29513456478714944\n",
      "Average loss at step  52500 :  0.44422731757164\n",
      "Average loss at step  52550 :  0.3042130896449089\n",
      "Average loss at step  52600 :  0.32571120873093606\n",
      "Average loss at step  52650 :  0.22743332669138908\n",
      "Average loss at step  52700 :  0.3696932490170002\n",
      "Average loss at step  52750 :  0.2791983962059021\n",
      "Average loss at step  52800 :  0.394083551466465\n",
      "Average loss at step  52850 :  0.2783360579609871\n",
      "Average loss at step  52900 :  0.3606975619494915\n",
      "Average loss at step  52950 :  0.23078469008207322\n",
      "Average loss at step  53000 :  0.42814273238182066\n",
      "Average loss at step  53050 :  0.36471248462796213\n",
      "Average loss at step  53100 :  0.3988066916167736\n",
      "Average loss at step  53150 :  0.35734800770878794\n",
      "Average loss at step  53200 :  0.3526144389808178\n",
      "Average loss at step  53250 :  0.28433975256979466\n",
      "Average loss at step  53300 :  0.3898841281235218\n",
      "Average loss at step  53350 :  0.23669298470020295\n",
      "Average loss at step  53400 :  0.35591586083173754\n",
      "Average loss at step  53450 :  0.2765528477728367\n",
      "Average loss at step  53500 :  0.45922469705343244\n",
      "Average loss at step  53550 :  0.23577264413237572\n",
      "Average loss at step  53600 :  0.4164920854568481\n",
      "Average loss at step  53650 :  0.2798980113863945\n",
      "Average loss at step  53700 :  0.2615729312598705\n",
      "Average loss at step  53750 :  0.24476189255714417\n",
      "Average loss at step  53800 :  0.22299262553453444\n",
      "Average loss at step  53850 :  0.2874743948876858\n",
      "Average loss at step  53900 :  0.2558770081400871\n",
      "Average loss at step  53950 :  0.2291669826209545\n",
      "Average loss at step  54000 :  0.23451109394431113\n",
      "Average loss at step  54050 :  0.403804085701704\n",
      "Average loss at step  54100 :  0.4490714682638645\n",
      "Average loss at step  54150 :  0.2692313560843468\n",
      "Average loss at step  54200 :  0.32299612134695055\n",
      "Average loss at step  54250 :  0.2969308233261108\n",
      "Average loss at step  54300 :  0.38495485424995424\n",
      "Average loss at step  54350 :  0.3342090792953968\n",
      "Average loss at step  54400 :  0.2560108613967895\n",
      "Average loss at step  54450 :  0.33383518919348715\n",
      "Average loss at step  54500 :  0.23344631731510163\n",
      "Average loss at step  54550 :  0.25931206315755845\n",
      "Average loss at step  54600 :  0.3024758992344141\n",
      "Average loss at step  54650 :  0.3007948283851147\n",
      "Average loss at step  54700 :  0.39611472383141516\n",
      "Average loss at step  54750 :  0.39465172290802003\n",
      "Average loss at step  54800 :  0.3159121802449226\n",
      "Average loss at step  54850 :  0.2535982824862003\n",
      "Average loss at step  54900 :  0.30909828320145605\n",
      "Average loss at step  54950 :  0.23591557577252387\n",
      "Average loss at step  55000 :  0.2822136729955673\n",
      "Average loss at step  55050 :  0.2970488999783993\n",
      "Average loss at step  55100 :  0.2678668686747551\n",
      "Average loss at step  55150 :  0.2599652785807848\n",
      "Average loss at step  55200 :  0.22685668662190436\n",
      "Average loss at step  55250 :  0.2784970133006573\n",
      "Average loss at step  55300 :  0.2906369717419148\n",
      "Average loss at step  55350 :  0.2533694270253182\n",
      "Average loss at step  55400 :  0.2239720468223095\n",
      "Average loss at step  55450 :  0.19932109862565994\n",
      "Average loss at step  55500 :  0.3234716808795929\n",
      "Average loss at step  55550 :  0.21951091915369034\n",
      "Average loss at step  55600 :  0.22873214349150658\n",
      "Average loss at step  55650 :  0.2783171358704567\n",
      "Average loss at step  55700 :  0.24292138427495957\n",
      "Average loss at step  55750 :  0.2324406921863556\n",
      "Average loss at step  55800 :  0.21507519498467445\n",
      "Average loss at step  55850 :  0.3497754980623722\n",
      "Average loss at step  55900 :  0.19984660908579827\n",
      "Average loss at step  55950 :  0.38264003694057463\n",
      "Average loss at step  56000 :  0.25401509523391724\n",
      "Average loss at step  56050 :  0.41209201484918595\n",
      "Average loss at step  56100 :  0.27732293382287027\n",
      "Average loss at step  56150 :  0.34778628453612326\n",
      "Average loss at step  56200 :  0.21240893006324768\n",
      "Average loss at step  56250 :  0.3575309570133686\n",
      "Average loss at step  56300 :  0.3428764271736145\n",
      "Average loss at step  56350 :  0.33771474108099936\n",
      "Average loss at step  56400 :  0.22220703333616257\n",
      "Average loss at step  56450 :  0.30422337129712107\n",
      "Average loss at step  56500 :  0.3618087996542454\n",
      "Average loss at step  56550 :  0.29928176701068876\n",
      "Average loss at step  56600 :  0.2689858461916447\n",
      "Average loss at step  56650 :  0.3204522494971752\n",
      "Average loss at step  56700 :  0.32800153627991674\n",
      "Average loss at step  56750 :  0.25495849281549454\n",
      "Average loss at step  56800 :  0.3157115396857262\n",
      "Average loss at step  56850 :  0.3998553086817265\n",
      "Average loss at step  56900 :  0.251774487644434\n",
      "Average loss at step  56950 :  0.30706159859895704\n",
      "Average loss at step  57000 :  0.2555765476822853\n",
      "Average loss at step  57050 :  0.3448416508734226\n",
      "Average loss at step  57100 :  0.3561958782374859\n",
      "Average loss at step  57150 :  0.2806065656244755\n",
      "Average loss at step  57200 :  0.24571536377072334\n",
      "Average loss at step  57250 :  0.3037131881713867\n",
      "Average loss at step  57300 :  0.24895771861076355\n",
      "Average loss at step  57350 :  0.2375871393084526\n",
      "Average loss at step  57400 :  0.265859812349081\n",
      "Average loss at step  57450 :  0.2604000329971313\n",
      "Average loss at step  57500 :  0.26943353667855263\n",
      "Average loss at step  57550 :  0.29690868258476255\n",
      "Average loss at step  57600 :  0.252653923034668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  57650 :  0.2953867910802364\n",
      "Average loss at step  57700 :  0.23594455435872078\n",
      "Average loss at step  57750 :  0.338273511081934\n",
      "Average loss at step  57800 :  0.317104811668396\n",
      "Average loss at step  57850 :  0.2770362521708012\n",
      "Average loss at step  57900 :  0.2784011016786099\n",
      "Average loss at step  57950 :  0.2968102511763573\n",
      "Average loss at step  58000 :  0.2340497800707817\n",
      "Average loss at step  58050 :  0.25497250467538835\n",
      "Average loss at step  58100 :  0.20436914324760436\n",
      "Average loss at step  58150 :  0.21278134629130363\n",
      "Average loss at step  58200 :  0.221567442715168\n",
      "Average loss at step  58250 :  0.234193157851696\n",
      "Average loss at step  58300 :  0.20011682987213134\n",
      "Average loss at step  58350 :  0.27555200934410096\n",
      "Average loss at step  58400 :  0.2642353459447622\n",
      "Average loss at step  58450 :  0.39803224816918376\n",
      "Average loss at step  58500 :  0.20459269493818283\n",
      "Average loss at step  58550 :  0.26913427144289015\n",
      "Average loss at step  58600 :  0.24886073999106884\n",
      "Average loss at step  58650 :  0.23949936673045158\n",
      "Average loss at step  58700 :  0.3147451665997505\n",
      "Average loss at step  58750 :  0.4504773852229118\n",
      "Average loss at step  58800 :  0.21982066735625266\n",
      "Average loss at step  58850 :  0.21546598479151727\n",
      "Average loss at step  58900 :  0.321940144598484\n",
      "Average loss at step  58950 :  0.21196402847766876\n",
      "Average loss at step  59000 :  0.25161906227469444\n",
      "Average loss at step  59050 :  0.31077613912522795\n",
      "Average loss at step  59100 :  0.3045592990517616\n",
      "Average loss at step  59150 :  0.19881029203534126\n",
      "Average loss at step  59200 :  0.31400154635310173\n",
      "Average loss at step  59250 :  0.29053441375494005\n",
      "Average loss at step  59300 :  0.28368114069104194\n",
      "Average loss at step  59350 :  0.39166662231087684\n",
      "Average loss at step  59400 :  0.33907928615808486\n",
      "Average loss at step  59450 :  0.2007655967772007\n",
      "Average loss at step  59500 :  0.23466500163078308\n",
      "Average loss at step  59550 :  0.2539090259373188\n",
      "Average loss at step  59600 :  0.26344854772090914\n",
      "Average loss at step  59650 :  0.32565357998013494\n",
      "Average loss at step  59700 :  0.2601428873836994\n",
      "Average loss at step  59750 :  0.21279478192329407\n",
      "Average loss at step  59800 :  0.24977530777454376\n",
      "Average loss at step  59850 :  0.29660590887069704\n",
      "Average loss at step  59900 :  0.2555240570008755\n",
      "Average loss at step  59950 :  0.2900678083300591\n",
      "Average loss at step  60000 :  0.1902137914299965\n",
      "Nearest to 2: 1205, 7903, 3819, 10244, 3763, 6182, 766, 2061,\n",
      "Nearest to 9: 7075, 9497, 6065, 2677, 6279, 1345, 2976, 9060,\n",
      "Average loss at step  60050 :  0.2445330289006233\n",
      "Average loss at step  60100 :  0.3545816330611706\n",
      "Average loss at step  60150 :  0.33987777642905714\n",
      "Average loss at step  60200 :  0.2602945911884308\n",
      "Average loss at step  60250 :  0.23363930732011795\n",
      "Average loss at step  60300 :  0.19840032160282134\n",
      "Average loss at step  60350 :  0.19176480486989023\n",
      "Average loss at step  60400 :  0.22277901716530324\n",
      "Average loss at step  60450 :  0.2752265252172947\n",
      "Average loss at step  60500 :  0.3066681972146034\n",
      "Average loss at step  60550 :  0.2748862283676863\n",
      "Average loss at step  60600 :  0.23419793277978898\n",
      "Average loss at step  60650 :  0.20638834565877914\n",
      "Average loss at step  60700 :  0.28217991650104524\n",
      "Average loss at step  60750 :  0.22423901468515395\n",
      "Average loss at step  60800 :  0.2294862761348486\n",
      "Average loss at step  60850 :  0.30945234254002574\n",
      "Average loss at step  60900 :  0.26470845252275466\n",
      "Average loss at step  60950 :  0.2892600744962692\n",
      "Average loss at step  61000 :  0.2184880404919386\n",
      "Average loss at step  61050 :  0.2977191489934921\n",
      "Average loss at step  61100 :  0.2616934703290463\n",
      "Average loss at step  61150 :  0.3239817854017019\n",
      "Average loss at step  61200 :  0.2900057479739189\n",
      "Average loss at step  61250 :  0.32182895578444004\n",
      "Average loss at step  61300 :  0.38380888134241103\n",
      "Average loss at step  61350 :  0.24813534520566463\n",
      "Average loss at step  61400 :  0.24455230236053466\n",
      "Average loss at step  61450 :  0.2763705751299858\n",
      "Average loss at step  61500 :  0.22114092916250228\n",
      "Average loss at step  61550 :  0.22713145062327386\n",
      "Average loss at step  61600 :  0.2598977980017662\n",
      "Average loss at step  61650 :  0.30893973156809806\n",
      "Average loss at step  61700 :  0.29539059519767763\n",
      "Average loss at step  61750 :  0.25062499597668647\n",
      "Average loss at step  61800 :  0.22655703999102117\n",
      "Average loss at step  61850 :  0.20849137395620346\n",
      "Average loss at step  61900 :  0.19178394988179207\n",
      "Average loss at step  61950 :  0.25668170243501665\n",
      "Average loss at step  62000 :  0.22009871184825897\n",
      "Average loss at step  62050 :  0.2173062027990818\n",
      "Average loss at step  62100 :  0.20158881835639478\n",
      "Average loss at step  62150 :  0.24557671293616296\n",
      "Average loss at step  62200 :  0.21375526294112204\n",
      "Average loss at step  62250 :  0.23137668266892433\n",
      "Average loss at step  62300 :  0.20471191816031933\n",
      "Average loss at step  62350 :  0.21278221428394317\n",
      "Average loss at step  62400 :  0.2557519268989563\n",
      "Average loss at step  62450 :  0.5095234783738851\n",
      "Average loss at step  62500 :  0.2769079229235649\n",
      "Average loss at step  62550 :  0.22056640028953553\n",
      "Average loss at step  62600 :  0.20653055250644684\n",
      "Average loss at step  62650 :  0.2994248075783253\n",
      "Average loss at step  62700 :  0.3297696927189827\n",
      "Average loss at step  62750 :  0.32069055452942846\n",
      "Average loss at step  62800 :  0.39294470101594925\n",
      "Average loss at step  62850 :  0.2481007968634367\n",
      "Average loss at step  62900 :  0.21903279535472392\n",
      "Average loss at step  62950 :  0.18855036914348602\n",
      "Average loss at step  63000 :  0.2157248641550541\n",
      "Average loss at step  63050 :  0.30118178345263\n",
      "Average loss at step  63100 :  0.19257793121039868\n",
      "Average loss at step  63150 :  0.2028823685646057\n",
      "Average loss at step  63200 :  0.2586462029069662\n",
      "Average loss at step  63250 :  0.264694260507822\n",
      "Average loss at step  63300 :  0.24157169967889786\n",
      "Average loss at step  63350 :  0.19637382663786412\n",
      "Average loss at step  63400 :  0.2553867131471634\n",
      "Average loss at step  63450 :  0.2118548633158207\n",
      "Average loss at step  63500 :  0.23230500742793084\n",
      "Average loss at step  63550 :  0.19202689766883851\n",
      "Average loss at step  63600 :  0.2818424907326698\n",
      "Average loss at step  63650 :  0.22825718633830547\n",
      "Average loss at step  63700 :  0.31761972874403\n",
      "Average loss at step  63750 :  0.2204907189309597\n",
      "Average loss at step  63800 :  0.24148408368229865\n",
      "Average loss at step  63850 :  0.2422877812385559\n",
      "Average loss at step  63900 :  0.5138812100887299\n",
      "Average loss at step  63950 :  0.24171219997107982\n",
      "Average loss at step  64000 :  0.19739075630903244\n",
      "Average loss at step  64050 :  0.23216005779802798\n",
      "Average loss at step  64100 :  0.2680258908867836\n",
      "Average loss at step  64150 :  0.16897834725677968\n",
      "Average loss at step  64200 :  0.26860895022749903\n",
      "Average loss at step  64250 :  0.2559179401397705\n",
      "Average loss at step  64300 :  0.20558733746409416\n",
      "Average loss at step  64350 :  0.26201374784111975\n",
      "Average loss at step  64400 :  0.2786940938234329\n",
      "Average loss at step  64450 :  0.21536577254533767\n",
      "Average loss at step  64500 :  0.26420228198170664\n",
      "Average loss at step  64550 :  0.22138702884316444\n",
      "Average loss at step  64600 :  0.1602649401128292\n",
      "Average loss at step  64650 :  0.24339622408151626\n",
      "Average loss at step  64700 :  0.1638476711511612\n",
      "Average loss at step  64750 :  0.3236031596362591\n",
      "Average loss at step  64800 :  0.22786869660019873\n",
      "Average loss at step  64850 :  0.23889140099287032\n",
      "Average loss at step  64900 :  0.2705021515488625\n",
      "Average loss at step  64950 :  0.16527431607246398\n",
      "Average loss at step  65000 :  0.31721176847815513\n",
      "Average loss at step  65050 :  0.19788399174809457\n",
      "Average loss at step  65100 :  0.29575674667954444\n",
      "Average loss at step  65150 :  0.18687839061021805\n",
      "Average loss at step  65200 :  0.2140432371944189\n",
      "Average loss at step  65250 :  0.19157422468066215\n",
      "Average loss at step  65300 :  0.19560587778687477\n",
      "Average loss at step  65350 :  0.20518167324364187\n",
      "Average loss at step  65400 :  0.2639091111719608\n",
      "Average loss at step  65450 :  0.2712729761749506\n",
      "Average loss at step  65500 :  0.2611786310374737\n",
      "Average loss at step  65550 :  0.2272908027470112\n",
      "Average loss at step  65600 :  0.2408933997154236\n",
      "Average loss at step  65650 :  0.22942233689129352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  65700 :  0.22782692670822144\n",
      "Average loss at step  65750 :  0.19273047015070915\n",
      "Average loss at step  65800 :  0.225618404597044\n",
      "Average loss at step  65850 :  0.2339963100105524\n",
      "Average loss at step  65900 :  0.24669372603297235\n",
      "Average loss at step  65950 :  0.16412678614258766\n",
      "Average loss at step  66000 :  0.24801309309899808\n",
      "Average loss at step  66050 :  0.28042781427502633\n",
      "Average loss at step  66100 :  0.1986023509502411\n",
      "Average loss at step  66150 :  0.22672744564712047\n",
      "Average loss at step  66200 :  0.3175330502539873\n",
      "Average loss at step  66250 :  0.49226571261882784\n",
      "Average loss at step  66300 :  0.3225605984032154\n",
      "Average loss at step  66350 :  0.31824095636606214\n",
      "Average loss at step  66400 :  0.26531179875135424\n",
      "Average loss at step  66450 :  0.22624082818627358\n",
      "Average loss at step  66500 :  0.2574849942326546\n",
      "Average loss at step  66550 :  0.25049013309180734\n",
      "Average loss at step  66600 :  0.24675057597458364\n",
      "Average loss at step  66650 :  0.18377860859036446\n",
      "Average loss at step  66700 :  0.21630135983228682\n",
      "Average loss at step  66750 :  0.2290285824239254\n",
      "Average loss at step  66800 :  0.1807608824968338\n",
      "Average loss at step  66850 :  0.2666081826388836\n",
      "Average loss at step  66900 :  0.2510838650166988\n",
      "Average loss at step  66950 :  0.20961774930357932\n",
      "Average loss at step  67000 :  0.18721839345991612\n",
      "Average loss at step  67050 :  0.25296955421566963\n",
      "Average loss at step  67100 :  0.23793355107307435\n",
      "Average loss at step  67150 :  0.2360139075666666\n",
      "Average loss at step  67200 :  0.17616889096796512\n",
      "Average loss at step  67250 :  0.22061858467757703\n",
      "Average loss at step  67300 :  0.18622577212750913\n",
      "Average loss at step  67350 :  0.21296962946653367\n",
      "Average loss at step  67400 :  0.30172035068273545\n",
      "Average loss at step  67450 :  0.23041212119162083\n",
      "Average loss at step  67500 :  0.27355213403701784\n",
      "Average loss at step  67550 :  0.21898912385106087\n",
      "Average loss at step  67600 :  0.19853060886263849\n",
      "Average loss at step  67650 :  0.19106235943734645\n",
      "Average loss at step  67700 :  0.29201253011822703\n",
      "Average loss at step  67750 :  0.21983308106660843\n",
      "Average loss at step  67800 :  0.2167925627529621\n",
      "Average loss at step  67850 :  0.20579276904463767\n",
      "Average loss at step  67900 :  0.1903421637415886\n",
      "Average loss at step  67950 :  0.3762810906767845\n",
      "Average loss at step  68000 :  0.3412289002537727\n",
      "Average loss at step  68050 :  0.2569600699841976\n",
      "Average loss at step  68100 :  0.17718064710497855\n",
      "Average loss at step  68150 :  0.19970676466822623\n",
      "Average loss at step  68200 :  0.23785649865865707\n",
      "Average loss at step  68250 :  0.1980453187227249\n",
      "Average loss at step  68300 :  0.16886706948280333\n",
      "Average loss at step  68350 :  0.24022703677415846\n",
      "Average loss at step  68400 :  0.1949030777066946\n",
      "Average loss at step  68450 :  0.21258193299174308\n",
      "Average loss at step  68500 :  0.266824324131012\n",
      "Average loss at step  68550 :  0.20113444544374942\n",
      "Average loss at step  68600 :  0.2144185321778059\n",
      "Average loss at step  68650 :  0.2089379458129406\n",
      "Average loss at step  68700 :  0.20936249576509\n",
      "Average loss at step  68750 :  0.19724493652582167\n",
      "Average loss at step  68800 :  0.23517539814114571\n",
      "Average loss at step  68850 :  0.24979231148958206\n",
      "Average loss at step  68900 :  0.2370009219646454\n",
      "Average loss at step  68950 :  0.19553919948637485\n",
      "Average loss at step  69000 :  0.20343410089612007\n",
      "Average loss at step  69050 :  0.26143760457634924\n",
      "Average loss at step  69100 :  0.1914319720119238\n",
      "Average loss at step  69150 :  0.20330192908644676\n",
      "Average loss at step  69200 :  0.1941205656528473\n",
      "Average loss at step  69250 :  0.20498657897114753\n",
      "Average loss at step  69300 :  0.21142195656895638\n",
      "Average loss at step  69350 :  0.2541614183783531\n",
      "Average loss at step  69400 :  0.25042583487927916\n",
      "Average loss at step  69450 :  0.2134615069627762\n",
      "Average loss at step  69500 :  0.20281442783772946\n",
      "Average loss at step  69550 :  0.2582753586769104\n",
      "Average loss at step  69600 :  0.1506295268982649\n",
      "Average loss at step  69650 :  0.2011548912525177\n",
      "Average loss at step  69700 :  0.2138043025135994\n",
      "Average loss at step  69750 :  0.21206723608076572\n",
      "Average loss at step  69800 :  0.23542675085365772\n",
      "Average loss at step  69850 :  0.26771045982837677\n",
      "Average loss at step  69900 :  0.18157276153564453\n",
      "Average loss at step  69950 :  0.1690507075190544\n",
      "Average loss at step  70000 :  0.18105125308036804\n",
      "Nearest to 2: 1205, 7903, 3819, 10244, 3763, 6182, 766, 2061,\n",
      "Nearest to 9: 7075, 9497, 6279, 6065, 2677, 2976, 1345, 9060,\n",
      "Average loss at step  70050 :  0.2800323788821697\n",
      "Average loss at step  70100 :  0.2039712706208229\n",
      "Average loss at step  70150 :  0.18686050184071065\n",
      "Average loss at step  70200 :  0.22210004389286042\n",
      "Average loss at step  70250 :  0.2457064187526703\n",
      "Average loss at step  70300 :  0.17977859780192376\n",
      "Average loss at step  70350 :  0.15268763214349745\n",
      "Average loss at step  70400 :  0.31963346362113954\n",
      "Average loss at step  70450 :  0.22095055371522904\n",
      "Average loss at step  70500 :  0.198016147762537\n",
      "Average loss at step  70550 :  0.22796939447522163\n",
      "Average loss at step  70600 :  0.39750840947031973\n",
      "Average loss at step  70650 :  0.2104785469174385\n",
      "Average loss at step  70700 :  0.20419781126081943\n",
      "Average loss at step  70750 :  0.18493443675339222\n",
      "Average loss at step  70800 :  0.1832390733063221\n",
      "Average loss at step  70850 :  0.23244585067033768\n",
      "Average loss at step  70900 :  0.20870642878115178\n",
      "Average loss at step  70950 :  0.17309131555259227\n",
      "Average loss at step  71000 :  0.17331113815307617\n",
      "Average loss at step  71050 :  0.2155082768201828\n",
      "Average loss at step  71100 :  0.18188244737684728\n",
      "Average loss at step  71150 :  0.18480464234948157\n",
      "Average loss at step  71200 :  0.21768368616700173\n",
      "Average loss at step  71250 :  0.22241733074188233\n",
      "Average loss at step  71300 :  0.20034213438630105\n",
      "Average loss at step  71350 :  0.20318389371037482\n",
      "Average loss at step  71400 :  0.17120839089155196\n",
      "Average loss at step  71450 :  0.20127672016620635\n",
      "Average loss at step  71500 :  0.17278060503304005\n",
      "Average loss at step  71550 :  0.2697365526854992\n",
      "Average loss at step  71600 :  0.237355153337121\n",
      "Average loss at step  71650 :  0.24487646035850047\n",
      "Average loss at step  71700 :  0.24943314284086227\n",
      "Average loss at step  71750 :  0.24944059140980243\n",
      "Average loss at step  71800 :  0.19028297901153565\n",
      "Average loss at step  71850 :  0.22513045459985734\n",
      "Average loss at step  71900 :  0.16298655413091181\n",
      "Average loss at step  71950 :  0.16384973853826523\n",
      "Average loss at step  72000 :  0.19515242978930472\n",
      "Average loss at step  72050 :  0.17718191161751748\n",
      "Average loss at step  72100 :  0.15272437766194344\n",
      "Average loss at step  72150 :  0.19310356602072715\n",
      "Average loss at step  72200 :  0.1553081564605236\n",
      "Average loss at step  72250 :  0.19894001804292202\n",
      "Average loss at step  72300 :  0.1859277117997408\n",
      "Average loss at step  72350 :  0.21312906146049498\n",
      "Average loss at step  72400 :  0.21172637283802032\n",
      "Average loss at step  72450 :  0.21830973669886589\n",
      "Average loss at step  72500 :  0.20340630993247033\n",
      "Average loss at step  72550 :  0.21241868466138839\n",
      "Average loss at step  72600 :  0.22184668645262717\n",
      "Average loss at step  72650 :  0.17204925939440727\n",
      "Average loss at step  72700 :  0.2250834383815527\n",
      "Average loss at step  72750 :  0.18389367178082466\n",
      "Average loss at step  72800 :  0.2705931979417801\n",
      "Average loss at step  72850 :  0.17657521307468416\n",
      "Average loss at step  72900 :  0.24389778546988963\n",
      "Average loss at step  72950 :  0.16508151911199093\n",
      "Average loss at step  73000 :  0.23372690871357918\n",
      "Average loss at step  73050 :  0.17444878071546555\n",
      "Average loss at step  73100 :  0.2394013074785471\n",
      "Average loss at step  73150 :  0.21442706927657126\n",
      "Average loss at step  73200 :  0.18300423912703992\n",
      "Average loss at step  73250 :  0.18057675652205943\n"
     ]
    }
   ],
   "source": [
    "num_steps = len(batches)\n",
    "reset_batch_gen()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # Open a writer to write summaries.\n",
    "    writer = tf.summary.FileWriter(FLAGS.log_dir, session.graph)\n",
    "\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print('Initialized')\n",
    "\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "#         print(step)\n",
    "        inputs, contexts = generate_batch()\n",
    "        feed_dict = {train_inputs: inputs, train_contexts: contexts}\n",
    "\n",
    "        # Define metadata variable.\n",
    "        run_metadata = tf.RunMetadata()\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        # Also, evaluate the merged op to get all summaries from the returned \"summary\" variable.\n",
    "        # Feed metadata variable to session for visualizing the graph in TensorBoard.\n",
    "        _, summary, loss_val = session.run(\n",
    "            [optimizer, merged, loss],\n",
    "            feed_dict=feed_dict,\n",
    "            run_metadata=run_metadata)\n",
    "        average_loss += loss_val\n",
    "\n",
    "        # Add returned summaries to writer in each step.\n",
    "        writer.add_summary(summary, step)\n",
    "        # Add metadata to visualize the graph for the last run.\n",
    "        if step == (num_steps - 1):\n",
    "          writer.add_run_metadata(run_metadata, 'step%d' % step)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "          if step > 0:\n",
    "            average_loss /= 50\n",
    "          # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "          print('Average loss at step ', step, ': ', average_loss)\n",
    "          average_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "          sim = similarity.eval()\n",
    "          for i in range(valid_size):\n",
    "            valid_word = valid_examples[i]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "              close_word = nearest[k]\n",
    "              log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    final_embeddings = normalized_embeddings.eval()\n",
    "    # Write corresponding labels for the embeddings.\n",
    "\n",
    "    with open(FLAGS.log_dir + '/metadata.tsv', 'w') as f:\n",
    "        for i in range(vocab_size):\n",
    "          f.write(str(i+start_id) + '\\n')\n",
    "\n",
    "    # Save the model for checkpoints.\n",
    "    saver.save(session, os.path.join(FLAGS.log_dir, 'model.ckpt'))\n",
    "\n",
    "    # Create a configuration for visualizing embeddings with the labels in TensorBoard.\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_conf = config.embeddings.add()\n",
    "    embedding_conf.tensor_name = embeddings.name\n",
    "    embedding_conf.metadata_path = os.path.join(FLAGS.log_dir, 'metadata.tsv')\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Visualize the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "10400\n",
      "10046->3\n",
      "10053->2\n",
      "10054->23\n",
      "10058->19\n",
      "10063->15\n",
      "10069->3\n",
      "10084->24\n",
      "10098->5\n",
      "10100->25\n",
      "10109->32\n",
      "10115->3\n",
      "10140->7\n",
      "10142->19\n",
      "10143->1\n",
      "10146->10\n",
      "10158->22\n",
      "10164->3\n",
      "10167->11\n",
      "10182->3\n",
      "10215->6\n",
      "10219->36\n",
      "10225->20\n",
      "10230->8\n",
      "10245->8\n",
      "10257->19\n",
      "10258->24\n",
      "10260->6\n",
      "10261->3\n",
      "10269->18\n",
      "10272->25\n",
      "10291->2\n",
      "10297->6\n",
      "10298->24\n",
      "10303->5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAEyCAYAAABzmvKXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzs3Xtc1HW++PHXR8YbXqEglFFQQUEuouL1V2bes10rtdTs6CJm6ba1burSemrNk2taaXWszvHWtup62bJwzUrFvKbHvIC3NDVBUVNEEe868P79MTAxwOAMzg34PB+P72OYz3zny/ujzHu+3+/npkQETdM0rWzVPB2ApmlaRaCTpaZpmh10stQ0TbODTpaapml20MlS0zTNDjpZapqm2UEnS03TNDvoZKlpmmYHnSw1TdPsYPB0APa6//77JTQ01NNhaJpWyezevfuCiATcbb8KkyxDQ0PZtWuXp8PQNK2SUUpl2LOfvgzXNE2zg06WmqZpdtDJUnObUaNGERgYSHR0tKXs4sWL9O7dm/DwcHr37s2lS5cAOHz4MF26dKFmzZq88847VsfJyclh8ODBREREEBkZyfbt261ef/fdd1FKceHCBddXSqsydLLU3OZ3v/sd33zzjVXZW2+9Rc+ePTl69Cg9e/bkrbfeAsDf358PPviACRMmlDjOyy+/TL9+/Th8+DBpaWlERkZaXjt16hRr166ladOmrq2MVuXoZKm5Tbdu3fD397cqS05OZuTIkQCMHDmSL7/8EoDAwEA6dOhA9erVrfa/fPkymzdvJjExEYAaNWrQsGFDy+vjx49n5syZKKVcWRWtCtLJUvOoc+fO0ahRIwCCgoI4d+5cmfufOHGCgIAAEhISaNu2LaNHj+batWuAOfEGBwfTpk0bl8etVT06WWpeQyl11zNCk8nEnj17GDt2LHv37qVOnTq89dZbXL9+nb/97W9MnTrVTdFqVY1OlppHPfDAA5w9exaAs2fPEhgYWOb+RqMRo9FIp06dABg8eDB79uzh+PHjnDhxgjZt2hAaGkpmZibt2rXjl19+cXkdtKpBJ0vNowYMGMCnn34KwKeffsrjjz9e5v5BQUE0adKEI0eOAJCSkkLr1q2JiYnh/PnzpKenk56ejtFoZM+ePQQFBTk9Zkda9UWEl156ibCwMGJjY9mzZ4/lPT4+PsTFxREXF8eAAQMs5SkpKbRr1464uDgefPBBjh075vQ6aOUgIhVia9++vWgV29ChQyUoKEgMBoMEBwfL/Pnz5cKFC9KjRw8JCwuTnj17SnZ2toiInD17VoKDg6VevXrSoEEDCQ4OlsuXL4uIyN69e6V9+/YSExMjjz/+uFy8eLHE7woJCZGsrCyX1GPTpk2ye/duiYqKspRNnDhRpk+fLiIi06dPl0mTJomIyFdffSX9+vWT/Px82b59u3Ts2NHynjp16pR6/PDwcDl06JCIiHz44YcycuRIl9RDMwN2iR05qMIMd9QqvqVLl5ZanpKSUqIsKCiIzMzMUvePi4srdejrEmAycBJomp7Ot8Dw8odrU7du3UhPT7cqS05OZuPGjYC5Vb979+7MmDGD5ORkRowYgVKKzp07k5OTw9mzZy2NWqVRSpGbmwuYW/8bN27sglpojtLJUqsUlgBjgOsFzzMKnoNrEmZxtlr1T58+TZMmTSz7GY1GTp8+TaNGjbh58ybx8fEYDAaSkpJ44oknAJg/fz79+/endu3a1K9fnx07drihBtrd6HuWWqUwmV8TZaHrBeXuZk+rPkBGRga7du3in//8J3/84x85fvw4ALNnz2bNmjVkZmaSkJDAn/70J1eHrNlBJ0utUjjpYLmz2WrVDw4O5tSpU5b9MjMzCQ4OtrwG0Lx5c7p3787evXvJysoiLS3N0to/ZMgQvv/+ezfVQiuLTpZapWBrcKO7Bj3aatUfMGAA//jHPxARduzYQYMGDWjUqBGXLl3i1q1bAFy4cIFt27bRunVr/Pz8uHz5Mj/99BMA69atsxrOqXmOvmepVQrTsL5nCeBbUO5sw4YNY+PGjVy4cAGj0cgbb7xBUlISTz/9NAsWLCAkJIQVK1YA0L9/f9asWUNYWBi+vr588sknAPz44488//zzVKtWjfz8fJKSkmjdujUA8+bNY9CgQVSrVg0/Pz8WLlzoglpojlLmlnPvFx8fL3ryX60sVq3hmBOlOxp3nM50BfIuAibAAD7+YKjn6agqLaXUbhGJv9t++sxSqzSGU0GTY1GmK5CXBRSexJgKnqMTpofpe5aa5k3yLvJroiwkBeWaJ+lkqWlexeRgueYuOllWUI6MT16yZAmxsbHExMTQtWtX0tLSLO95//33iY6OJioqivfee89SnpqaSufOnYmLiyM+Pp6dO3e6r3JVmq07Y/qOmafpZFlBOTLreLNmzdi0aRP79+/ntddeY8wY89iWAwcOMG/ePHbu3ElaWhqrV6+2TNowadIk/vrXv5KamsrUqVOZNGmSeytYVfn4A8U7tKuCcs2TdLKsoByZdbxr1674+fkB0LlzZ8uY6x9//JFOnTrh6+uLwWDg4YcfZuXKlYAen+wxhnrgE8CvZ5IG83PduONx+ty+ErFn1vEFCxbw6KOPAhAdHc3kyZPJzs6mdu3arFmzhvh4cw+K9957j759+zJhwgTy8/P1KBJ3MtTTydEL6WRZSZU2Pvm7775jwYIFbN26FYDIyEj+/Oc/06dPH+rUqUNcXBw+Pj4AfPzxx8yePZtBgwaxYsUKEhMTWb9+vdvroWnewimX4UqphUqp80qpA0XKpiilTiulUgu2/kVee1UpdUwpdUQp1dcZMWhlzzq+b98+Ro8eTXJyMvfdd5+lPDExkd27d7N582b8/Pxo2bIlYB6yN3DgQACeeuop3cCjVXnOumf5d6BfKeWzRSSuYFsDoJRqDQwFogre85FSysdJcVRptsYnnzx5koEDB7Jo0SJLMix0/vx5yz4rV67kmWeeAaBx48Zs2rQJgA0bNhAeHu6uamiaV3LKZbiIbFZKhdq5++PAMhG5BZxQSh0DOgLbnRFLVeHI+OSpU6eSnZ3NuHHjADAYDJbJcwcNGkR2djbVq1fnww8/tCwrO2/ePF5++WVMJhO1atVi7ty5nqmopnkJp40NL0iWq0UkuuD5FOB3QC6wC3hFRC4ppeYAO0RkccF+C4CvReSzUo45hoI5XJs2bdo+IyPDKbFqd3fm4HKObpzCzdxMatU3Et59Co2jhng6LE1zOnvHhruy69DHQAsgDjgLvOvoAURkrojEi0h8QECAs+PTbDhzcDkH17zIzdxTgHAz9xQH17zImYPLPR2apnmMy5KliJwTkTwRyQfmYb7UBjgNNCmyq7GgTPMSRzdOId90w6os33SDoxuneCYgTfMCLkuWSqmiKzI9CRS2lK8ChiqlaiqlmgHhgG5q9SI3c0tfKMxWuaZVBU5p4FFKLQW6A/crpTKBvwLdlVJxmKdQSQeeBxCRg0qpFcAhzLMD/F5E8pwRh+YcteobCy7BS5ZrWlXlrNbwYaUULyhj/2m4ZhJrzQnCu0/h4JoXrS7FqxlqE959iueC0jQP02PDtRIaRw0hqv8catVvAihq1W9CVP85ujVcq9L0cEetVI2jhujkqGlF6DNLTdM0O+hkqWmaZgedLDVN0+ygk6WmaZoddLLUNE2zg06WmqZpdtDJUtO8nCMreR4+fJguXbpQs2ZN3nnnnRLHysvLo23btvzmN7+xlCUmJtKmTRtiY2MZPHgwV69edX2lKiCdLDXNyzmykqe/vz8ffPABEyZMKPVY77//PpGRkVZls2fPJi0tjX379tG0aVPmzJnjmopUcDpZapqXc2Qlz8DAQDp06ED16tVLHCczM5OvvvqK0aNHW5XXr18fABHhxo0bJdZu0sx0stQqJUcuXZcsWUJsbCwxMTF07dqVtLQ0AI4cOUJcXJxlq1+/Pu+99x4AEydOJCIigtjYWJ588klycnLcWj97VvIs7o9//CMzZ86kWrWSH/uEhASCgoI4fPgwf/jDH5web2Wgk6UNjnzYRISXXnqJsLAwYmNj2bNnj+U9Pj4+lg/bgAEDSvyel156ibp167q+QlWMI5euzZo1Y9OmTezfv5/XXnuNMWPGANCqVStSU1NJTU1l9+7d+Pr68uSTTwLQu3dvDhw4wL59+2jZsiXTp093bwWLKG0lz+JWr15NYGAg7du3L/X1Tz75hDNnzhAZGcny5XqS59LoZGmDIx+2r7/+mqNHj3L06FHmzp3L2LFjLe+pXbu25QO3atUqq+Pt2rXLknA153Lk0rVr1674+fkB0LlzZzIzS87bmZKSQosWLQgJCQGgT58+GAyGMt/jSmWt5Fmabdu2sWrVKkJDQxk6dCgbNmzg2WeftdrHx8eHoUOH8vnnn7ss7opMJ0sbHPmwJScnM2LECJRSdO7cmZycHMsfsi15eXlMnDiRmTNnuqYCWgn2XLouWLCARx99tET5smXLGDastJkIYeHChaW+x5VsreRpy/Tp08nMzCQ9PZ1ly5bRo0cPFi9ejIhw7NgxwHyFtGrVKiIiIlwef0WkZx1ygK0P2+nTp2nS5NeVMoxGI6dPn6ZRo0bcvHmT+Ph4DAYDSUlJPPHEEwDMmTOHAQMGWI6nuVdpl67fffcdCxYsYOvWrVblt2/fZtWqVaVeak+bNg2DwcDw4cNdFqsjK3n+8ssvxMfHk5ubS7Vq1Xjvvfc4dOiQpRGnOBFh5MiR5ObmIiK0adOGjz/+2GV1qch0siwne+4TAWRkZBAcHMzPP/9Mjx49iImJoXbt2vzrX/9i48aNrg9Usyi8dG3UqFGJS9d9+/YxevRovv76a+677z6r93399de0a9eOBx54wKr873//O6tXryYlJcWlLchLly4ttTwlJaVEWVBQ0F1vCXTv3p3u3bsDYMqvxtcbtiGAAmr5QA2dFUqlL8MdYOs+UXBwMKdO/boMQ2ZmJsHBwZbXAJo3b0737t3Zu3cve/fu5dixY4SFhREaGsr169cJCwtzc22qHluXridPnmTgwIEsWrSIli1blnjf0qVLS1yCf/PNN8ycOZNVq1bh6+vr+uBd4LYJbuSZ130B8+ONPHO5VgoRqRBb+/btxd1OnDghUVFRlucTJkyQ6dOni4jI9OnTZeLEiSIisnr1aunXr5/k5+fL9u3bpUOHDiIicvHiRbl586aIiGRlZUlYWJgcPHiwxO+pU6eOq6tS5QwdOlSCgoLEYDBIcHCwzJ8/Xy5cuCA9evSQsLAw6dmzp2RnZ4uISGJiojRs2FDatGkjbdq0kaJ/a1evXhV/f3/JycmxOn6LFi3EaDRa3vP888+7tX7OcPmmSE4p2+Wbno7MvYBdYkcO8ngStHdzd7J05MOWn58v48aNk+bNm0t0dLT88MMPIiKybds2iY6OltjYWImOjpb58+eX+rt0stQ8obREWbhVJfYmS2Xe1/vFx8fLrl27PB2GUyw5dIjJW7dyMjeXpvXrM+3BBxneurWnw9IckJGRwf79+7l+/Tq+vr7ExMRYuhVVFLm3fr0EL0oB9Wu6OxrPUUrtFpH4u+2nb+W62ZJDhxizdi3XTeYbQxm5uYxZuxZAJ8wKIiMjg127dpGXZ17B+fr16xR+kVekhFnLx3yPsrRyrSTdwONmk7dutSTKQtdNJiYX666iea/9+/dbEmWhvLw89u/f76GIyqeGAWr7mM8kwfxYW7eG26STpZudzM11qLwicMY47EKlTSH2u9/9jmbNmlmGjaamprqnYjZcv37doXJvVsNgvuRuUNP8qBOlbTpZullTG52DbZVXBM4Yh12otCnEAN5++23LsNG4uDjXVcYOtroKVdQuRJp9dLJ0s2kPPoivwfrr29dgYNqDD3ooonvnrHHYtqYQ8zYxMTH4+Fjf2PPx8SEmJsZDEWnuoJOlmw1v3Zq5ffoQUr8+CgipX5+5ffpUusad8ozDLmsKscmTJxMbG8v48eO5deuW6wK3Q0hICPHx8ZYzSV9fX+Lj4ytU447mOH2HwgOGt25d6ZJjWewZh110CrHiw0CnT59OUFAQt2/fZsyYMcyYMYPXX3/dXeGXKiQkRCfHKkafWWouUdYUYoXjsJOTky3jsMuaQqxRo0YopahZsyYJCQns3LnT/RXSqjydLDWXcHQctq0pxABL0hURvvzyS6tWd01zF30Zrt0zR6YQmzp1KtnZ2YwbNw4Ag8HA3UZmDR8+nKysLESEuLg4/ud//sflddK04pwy3FEptRD4DXBeRKILyvyB5UAokA48LSKXlPnm1ftAf+A68DsR2VPacYuqTMMdNccsWbKEyZMnc/LkSZo2bcq0adNcOn+kVrXYO9zRWZfhfwf6FStLAlJEJBxIKXgO8CgQXrCNAfRMo5pNS5YsYcyYMWRkZCAiZGRkMGbMGJYsWeLp0LQqxinJUkQ2AxeLFT8OfFrw86fAE0XK/1Ew4ccOoKFSSk8XrpVq8uTJJUbGXL9+ncmTJ3soIq2qcmUDzwMiUrgQzS9A4TTTwcCpIvtlFpSVoJQao5TapZTalZWV5bpIKxhnDS/Myclh8ODBREREEBkZyfbt2wH417/+RVRUFNWqVbvr/URXO3nypEPlmuYqbmkNL5gzzuGboyIyV0TiRSQ+ICDABZFVTM4aXvjyyy/Tr18/Dh8+TFpammWYYXR0NCtXrqRbt27uq5QNTZs2dahc01zFlcnyXOHldcHj+YLy00CTIvsZC8o0OzljeOHly5fZvHkziYmJANSoUYOGDRsCEBkZSatWrdxSl7uZNm1aiTHXvr6+TJs2zUMRaVWVK5PlKmBkwc8jgeQi5SOUWWfgcpHLda2cHB1eeOLECQICAkhISKBt27aMHj2aa9euuTVmewwfPpy5c+cSEhKCUoqQkBDmzp2rW8M1t3NKslRKLQW2A62UUplKqUTgLaC3Uuoo0KvgOcAa4GfgGDAPGOeMGLRflTW8cMaMGQCYTCb27NnD2LFj2bt3L3Xq1LFcunub4cOHk56eTn5+Punp6TpRah7hlE7pIlL66vPQs5R9Bfi9M36v9itHl3k1Go0YjUY6deoEwODBg702WWqaN9DDHSsJR4cXBgUF0aRJE44cOQKY16BuXYUm99A0h9mzqpk3bJ5YCtdbOWuZ171790r79u0lJiZGHn/8cbl48aKIiKxcuVKCg4OlRo0aEhgYKH369PFIPTXNHdCrO2qapt2du4c7apXIEswD+qsVPOqBhZqzBkKEhoYSExNDXFwc8fG/5qfU1FQ6d+5sKffKafjsOf30hk1fhrvHYhHxFet/fN+Ccq3q2rRpk+zevVuioqIsZRMnTpTp06eLiMj06dNl0qRJIiKybds2yy2dNWvWSMeOHS3vCQkJkaysrBLH7927t6xZs0ZERL766it5+OGHXVWVErDzMlyfWWpWJmOeCqqo6wXlWtXlrHWWbFFKkVuwwunly5dp3LixM8N3Cj2fpWbF1ohrPRJbK6486ywppejTpw9KKZ5//nnL8Nv33nuPvn37MmHCBPLz8/n+++/dUwkH6GSpWWkKZNgo1zRb7FlnCWDr1q0EBwdz/vx5evfuTUREBN26dePjjz9m9uzZDBo0iBUrVpCYmMj69evdXY0y6ctwzco0oPjq174F5Vr5OauB5P333yc6OpqoqCjee+89S/mQIUOIi4sjLi6O0NBQt6yt7ug6SwDBweYJxgIDA3nyySctDTmffvopAwcOBOCpp57yygYenSw1K8OBuUAImJfqLXiuBxjeG2fMFHXgwAHmzZvHzp07SUtLY/Xq1Rw7dgyA5cuXk5qaSmpqKoMGDbIkHldydCDEtWvXuHLliuXntWvXWr48GjduzKZNmwDYsGED4eHhLo/fYfa0AnnDplvDtYruxIkTVq3JLVu2lDNnzoiIyJkzZ6Rly5Yl3nPx4kVp3LixiIisWLFCRo0aZXlt6tSpMmPGDKv98/PzxWg0yk8//eTU2J0xEOL48eMSGxsrsbGx0rp1a3nzzTctx9+yZYu0a9dOYmNjpWPHjrJr1y6nxl8W7GwN93gStHfTyVKr6IonywYNGlh+zs/Pt3pe6O2335bExEQRETl06JCEh4fLhQsX5Nq1a9K5c2d58cUXrfbftGmT6M+KY+xNlrqBR9O8gD0NJJGRkfz5z3+mT58+1KlTh7i4OHx8fKzes3TpUoYNszWvjfc6c3A5RzdO4WZuJrXqGwnvPoXGUUM8HZYVfc9S0zykPA0kiYmJ7N69m82bN+Pn52d1T9BkMrFy5UqGDPGuJHM3Zw4u5+CaF7mZewoQbuae4uCaFzlzcLmnQ7Oik6WmeYijDSQA58+ft+yzcuVKnnnmGctr69evJyIiAqPR6KYaOMfRjVPIN92wKss33eDoximeCcgGfRmuaW4wbNgwNm7cyIULFzAajbzxxhskJSXx9NNPs2DBAkJCQlixYgUAU6dOJTs7m3HjzPNiGwwGy8JxgwYNIjs7m+rVq/Phhx9algIBWLZsWYW8BL+ZW/oIH1vlnqJnHdIqtFGjRrF69WoCAwM5cOAAYO6/OGTIENLT0wkNDWXFihX4+flx+PBhEhIS2LNnD9OmTWPChAmW44SGhlKvXj18fHysklNqaiovvPACN2/exGAw8NFHH9GxY0eP1NWWO3euYDJdRMSEUgYMBn+qV6/n6bDstunDyIJLcGu16jfh4d//6PLfX2VnHXJ1519vWiZWc6z/or+/Px988IFVkizqu+++IzU11er/ddKkSfz1r38lNTWVqVOnMmnSJNdVphzu3LnCnTtZiJgAEDFx504Wd+5c8XBk9gvvPoVqhtpWZdUMtQnvPsUzAdlQ6ZKlqzv/etMysZpjEzwEBgbSoUMHqlevbvfxvX2CB5PpIiVXmZaC8oqhcdQQovrPoVb9JoCiVv0mRPWf43Wt4ZXunmW3bt1IT0+3KktOTmbjxo2A+cPTvXt3ZsyYQdeuXS37FJ0d5ccff6RTp06WJVgffvhhVq5cyaRJkyxra2vey54JHoqrqBM8FJ5R2lvurRpHDfG65FhcpTuzLI2js6NER0ezZcsWsrOzuX79OmvWrOHUqZL3VDTvV1r/xdJs3bqVPXv28PXXX/Phhx+yefNmAMsED6dOnWL27NmWdda9hVKln+/YKtfKr0oky6LsWSa2aOfffv36ldr5V/NeZfVftKWiTvBgMPhjHsVflCoo15ypSiRLZ3f+1bybrf6LtlTkCR6qV69H9eoBljNJpQxUrx5QoVrDKwx7xkR6w+bIeNfiY3AnTJhgNf39xIkTRUQkIyNDWrRoIdu2bStxjHPnzln2adWqlVy6dMnq9Ycfflh++OEHu2PSXMORCR7Onj0rwcHBUq9ePWnQoIEEBwfL5cuXvXaCB809qKoTaThrmdgHH3xQIiMjJTY2VtavX28pr8rLxCYkJEhAQIDVF1F2drb06tVLwsLCpFevXpa1VxYvXiwxMTESHR0tXbp0kdTUVMt7Ll26JIMGDZJWrVpJRESEfP/992UeS9NcqcomS1cy3ciR29lH5XbWj3I7+6iYbuR4OiS3ctaiVSNGjJB58+aJiMitW7csZ+22juVJixcflJCQ/xWl3paQkP+VxYsPejokzcl0snQy040cuZ112JwoLdvhKpcw73VOxpycHAkNDZX8/PwS+9lzLHdavPig+PrOFnjbsvn6ztYJs5KxN1lWiQYeZ8i/nkVpnX/N5VWXo92yTpw4QUBAAAkJCbRt25bRo0dz7do1u4/lTpMnb+X6dev+itevm5g8eauNd2iVmU6W9sq30cnXVnkVZE+3LJPJxJ49exg7dix79+6lTp06lhFVdzuWu508metQuVa56WRpr2o2OvnaKrfBkbHrIsJLL71EWFgYsbGx7Nmzx/KeP//5z0RHRxMdHc3y5b/O+5eSkkK7du2Ii4vjwQcftAzTdBVHu2UZjUaMRiOdOnUCYPDgwZZ6lad/pCs1bVrfoXKtctPJ0k7VfAMorfOvudx+joxd//rrrzl69ChHjx5l7ty5jB07FoCvvvqKPXv2kJqayv/93//xzjvvWMYvjx07liVLlpCamsozzzzDm2++Wa762svRORmDgoJo0qQJR44cAczJvXXr1mUey1OmTXsQX1/rL0NfXwPTpj3ooYg0j7Lnxua9bEA6sB9IpeBGKuAPrAOOFjz63e04nm7gEXFea7i9jSRjxoyRf/7znyX2mzlzpkydOtVSPmrUKFm+fLllnx07doiIyN/+9jd59dVXyxVjaZzVLWvv3r3Svn17iYmJkccff9zSam7rWJ6kW8MrP7ylNbwgWd5frGwmkFTwcxIw427H8YZk6Sz2Llz12GOPyZYtWyyv9ejRQ3744Qf59ttvpWvXrnLt2jXJysqSZs2ayTvvvCMiIps3bxZ/f38JDg6WyMhIuXz5sptqpWkVk73J0lOX4Y8Dnxb8/CnwhIfi8Dr2NGz06dOH/v3707VrV4YNG0aXLl0sY9dnz57NmjVryMzMJCEhgT/96U/uCPue3blzhRs3Mrh+/Tg3bmRUqPkYtarBHclSgLVKqd1KqTEFZQ+IyNmCn38BHijtjUqpMUqpXUqpXVlZlbeLjq2GjeDgYKvZjjIzMy0TPkyePJnU1FTWrVuHiNCyZUuysrJIS0uzNJ4MGTLE66YUK01lmMBWq/zckSwfFJF2wKPA75VSVrPmFpwGl7q2hYjMFZF4EYkPCHCsIaUisdWwMWDAAP7xj38gIuzYsYMGDRrQqFEj8vLyyM7OBswtzvv27aNPnz74+flx+fJlfvrpJwDWrVtXIebfrAwT2GqVn8snvROR0wWP55VSXwAdgXNKqUYiclYp1Qg47+o4vIUjC1f179+fNWvWEBYWhq+vL5988gkAd+7c4aGHHgKgfv36LF68GIPB/F85b948Bg0aRLVq1fDz82PhwoWeqagDKssEtlrl5tIFy5RSdYBqInKl4Od1wFSgJ5AtIm8ppZIAfxEpc3ETvWCZfW4DNzGfpymgFlDDoxHd3Y0bGaUmRqUM1K4d4oGItKrE3gXLXH1m+QDwRUGDhQH4p4h8o5T6AVihlEoEMoCnXRxHlXAbKLr6shR57s0J02CSVcAVAAAgAElEQVTw586d4sNJ9QS2mndxabIUkZ+BNqWUZ2M+u9Sc6GYZ5d6cLAsnqq3Iy7lqlZ9eqKMSsXVDpSKsDG+e8VsnR8176eGOlYit3pmenY5C0yoHnSwrkVoOlmuaZj99GV6JFN6XrGit4ZpWEegzy0qmBlAfaFDwqBNl1eLIFIBLliwhNjaWmJgYunbtSlpaGgCnTp3ikUceoXXr1kRFRfH+++9bjjVlyhSCg4OJi4sjLi6ONWvWuLeCHqSTpaZVIo5MAdisWTM2bdrE/v37ee211xgzxjwa2WAw8O6773Lo0CF27NjBhx9+yKFDhyzHGz9+PKmpqaSmptK/f3/3Vc7DdLLUtEqkW7du+Ptb909NTk5m5MiRAIwcOZIvv/wSgK5du+Ln5wdA586dyczMBKBRo0a0a9cOgHr16hEZGcnp06fdVQWvpZOlplVyjq6TVFR6ejp79+61TM4CMGfOHGJjYxk1apTlkr4q0MlS00rhrOU/AHJzczEajbz44ouWstu3bzNmzBhatmxJREQEn3/+uVvqZc86SYWuXr3KoEGDeO+996hf37yUxtixYzl+/Dipqak0atSIV155xS1xewOdLDWtFM5Y/qPQa6+9RrduVpNtMW3aNAIDA/npp584dOgQDz/8sMvq4ug6SWCerGXQoEEMHz6cgQMHWh3Lx8eHatWq8dxzz7Fz506Xxe1tdLLUtFI4cu8vOTmZESNGoJSic+fO5OTkWJLT7t27OXfuHH369LE61sKFC3n11VcBqFatGvfff7/L6uLoOkkiQmJiIpGRkSUmjy6sF8AXX3xhdeZd2elkqWl2snXv7/Tp0zRp0sSyn9Fo5PTp0+Tn5/PKK6/wzjvvWB0nJycHMJ9xtmvXjqeeesppa6QXzpx/5MgRjEYjCxYsICkpiXXr1hEeHs769etJSkoCYOrUqWRnZzNu3Dji4uKIjzdPvLNt2zYWLVrEhg0bSnQRmjRpEjExMcTGxvLdd98xe/Zsp8RdEehO6ZpWDvYs//HRRx/Rv39/jEajVbnJZCIzM5OuXbsya9YsZs2axYQJE1i0aNE9x7V06dJSy1NSUkqUzZ8/n/nz55cof/DBBylt6sYrn63lzQM3MJ3zwxAciP+IMdQr+PKoCnSy1DQ7Fd77a9SokV3Lf2zfvp0tW7bw0UcfcfXqVW7fvk3dunWZPn06vr6+lnuBTz31FAsWLPBInex15bO1ZP1pJnLjFgCmzHNk/WkmAPUG9ynrrZWGvgzXNDs5uvzHkiVLOHnyJOnp6bzzzjuMGDGCt956C6UUv/3tb9m4cSNgvXa6t7o4ba4lURaSG7e4OG2uhyJyP50sNY9wpGvO4cOH6dKlCzVr1ixx/++bb76hVatWhIWFWVqnATZs2EC7du2Ijo5m5MiRmEyOLVHhyL2//v3707x5c8LCwnjuuef46KOP7nr8GTNmMGXKFGJjY1m0aBHvvvuuQ/G5m+l06Su/2CqvjFy6rIQz6WUlKpfNmzdTt25dRowYwYEDBwBz44G/vz9JSUm89dZbXLp0iRkzZnD+/HkyMjL48ssv8fPzY8KECQDk5eXRsmVL1q1bh9FopEOHDixdupSIiAhCQkJISUmhZcuWvP7664SEhJCYmOjJKlssASYDJ4GmwDRguEcjuruMtoMxZZZshDIYHyBk72ceiMh57F1WQp9Zah7hSNecwMBAOnToQPXq1a3237lzJ2FhYTRv3pwaNWowdOhQkpOTyc7OpkaNGpbuML1793Zbp++7WQKMwbyWihQ8jiko92b+k8egate0KlO1a+I/eYyNd1Q+OllqXsOeYXlF2eqyc//992MymSi8Evnss8+sGmA8aTJwvVjZ9YJyb1ZvcB8CZk3CYHwAlMJgfICAWZOqTOMOeEGyVEotVEqdV0odKFLmr5Rap5Q6WvDoB+Yzj9jYWEufsK1bt1qO8+mnnxIeHk54eLjlJjxAv379aNOmDVFRUbzwwgvk5eW5s3paOdnTNaes9y5btozx48fTsWNH6tWrh4+Pj5MjLJ+TDpZ7k3qD+xCy9zNanN9MyN7PqlSiBC9IlsDfgX7FypKAFBEJB1IKntOzZ0/S0tJITU1l4cKFjB49GjA3DLzxxhv83//9Hzt37uSNN96wNA6sWLGCtLQ0Dhw4QFZWFv/617/cVS/NQWUNyyuNrS47AF26dGHLli3s3LmTbt26WY1Q8aSmDpZr3sPjyVJENgMXixU/DhSeHn4KPAFQt25dy9nGtWvXLD9/++239O7dG39/f/z8/Ojdu7dlXG/hBAAmk4nbt2+X+2xFcz1bXXNs6dChA0ePHuXEiRPcvn2bZcuWMWDAAADOnze30t66dYsZM2bwwgsvuDZ4O00DfIuV+RaUa97N48nShgdEpHAQ6i+Y1x8HzONRIyIieOyxx1i4cCFg+95Vob59+xIYGEi9evUYPHiwWyrgLZzVRQfMrc9t27blN7/5jaVMRJg8eTItW7YkMjKSDz74wK64HOma88svv2A0Gpk1axZvvvkmRqOR3NxcDAYDc+bMoW/fvkRGRvL0008TFRUFwNtvv01kZCSxsbH89re/pUePHuX+N3Sm4cBcIATzsh8hBc+9vTVcw/zH7ukNCAUOFHmeU+z1S+3bt5eiNm3aJD179hQRkbffflv+67/+y/La1KlT5e2337ba/8aNGzJw4EBZu3atVCWbNm2S3bt3S1RUlKVs4sSJMn36dBERmT59ukyaNElERM6dOyc7d+6Uv/zlLyX+/URE3n33XRk2bJg89thjlrKFCxfKf/zHf0heXp7lGJ7205oLsqT/Pvnf9rtkSf998tOaC54OqcpKSEiQgIAAq7+/7Oxs6dWrl4SFhUmvXr3k4sWLIiKyePFiiYmJkejoaOnSpYukpqaKiPmz26FDB4mNjZXWrVvL66+/bjnWzz//LB07dpQWLVrI008/Lbdu3XI4RmCX2JGnvPXM8pxSqhFAwWOJnq/dunXj559/5sKFC2XeuypUq1YtHn/8cZKTk10cundxRhcdMP+bfvXVV5b7xIU+/vhjXn/9dapVq2Y5hicd/TqbLW9mcPWX2yBw9ZfbbHkzg6NfZ3s0rqrKGctc1KxZkw0bNljaK7755ht27NgBwJ///GfGjx/PsWPH8PPzc+mwUW9NlquAkQU/jwSSAY4dO2YZ4L9nzx5u3brFfffdR9++fVm7di2XLl3i0qVLrF27lr59+3L16lVLg4HJZOKrr74iIiLinoNz9aXtiRMn6NSpE2FhYQwZMoTbt2/fc8xFOdpFB+CPf/wjM2fOtCTFQsePH2f58uXEx8fz6KOPcvToUafG6qgf5pzBdNN6oIXppvDDnDMeiqhqc8YyF0op6tatC5jn2bxz5w5KKUSEDRs2WG6tFT2WK3g8WSqllgLbgVZKqUylVCLwFtBbKXUU6FXwnM8//5zo6Gji4uL4/e9/z/Lly1FK4e/vz2uvvUaHDh3o0KEDr7/+Ov7+/ly7do0BAwZYuhsFBgY65Ua/I9+W/v7+fPDBB5ZRJ8W9//77REZGWpW589vSni46q1evJjAwkPbt25d47datW9SqVYtdu3bx3HPPMWrUKFeFaper50r/YrFVrrlfeZa5yMvLs3yGe/fuTadOncjOzqZhw4YYDOb5gIq3VTidPdfq3rAVv2fpaSdOnLC6D9OyZUs5c+aMiIicOXNGWrZsabX/X//61xL3AU+dOiU9evSQlJQUy33A/Px8ue++++TOnTsiIvL9999Lnz59PBprUlKSBAcHS0hIiDzwwANSu3ZtGT58uIiItGrVSn7++WdL7PXr17+nWO/Vkv775H/b7SqxLem/z6NxVWXF//4aNGhg9XrDhg2tnm/YsEEiIiLkwoWS95ovXbok3bt3l/3790tWVpa0aNHC8trJkyetfo+9qOD3LJ3iNpALXC54dOW5hbMubd3xbeloF53p06eTmZlJeno6y5Yto0ePHixevBiAJ554gu+++w6ATZs2ebw/Y4cXG2OoZX2mbKil6PBiYw9FpBVXnmUuCjVs2JBHHnmEb775hvvuu4+cnBzLJCmltVU4U6VNlreBG5jH31LweAPXJsxC93pp60zO6KJTlqSkJD7//HNiYmJ49dVXS51M1p3CH72Ph/4zhLpBNUBB3aAaPPSfIYQ/WvKDp3mGo8tcZGVlWWaXv3HjBuvWrSMiIgKlFI888gifffZZiWO5hD2nn96wOXoZfllEckrZLjt0FNtcdWnristwV1q8WCQkREQp8+PixZ6OSPMmQ4cOlaCgIDEYDBIcHCzz58+XCxcuSI8ePSQsLEx69uwp2dnZIiKSmJgoDRs2lDZt2kibNm2k8DOflpYmcXFxEhMTI1FRUfLGG29Yjn/8+HHp0KGDtGjRQgYPHiw3b950OEbsvAz3WPLDPMTxCHAMSLrb/o4my9ISZeHmDMWT5YQJE6z6Lk6cONFq/9LuWRb67rvvrPouDh48WJYuXSoiIs8//7x8+OGHTorauRYvFvH1Nf8VFW6+vjphau6xePFiCQkJEaWUhISEyOJy/uF5dbIEfIDjQHOgBpAGtC7rPd50ZunIt+XZs2clODhY6tWrJw0aNJDg4GC5fNk6iuLJ0hnflu4QEmKdKAu3kBBPR6ZVdosXLxZfX1/BfIdNAPH19S1XwrQ3WXpk8l+lVBdgioj0LXj+KoCITLf1Hkcn/y28Z1lcbczZ2WtdOw9XTkLeLfCpCfWaQh3PdvS2pVo1c3osTinIz3d/PFrVERoaSkZGRonykJAQ0tPTHTqWt0/+GwwUnWAws6DMaWpgToyFzSyKCpIoLx83J0owP14+bi73Qk1tTJVjq1zTnOXkydIntbNV7gxe3RqulBqjlNqllNqVlZXl8PtrAPWBBgWPXp0owXxGKcVOySTfXO6Fpk0D32JT6Pj6mss1zZWa2vhGtlXuDJ5KlqeBJkWeGwvKrIjIXBGJF5H4gIAAtwXnMXm3HCv3sOHDYe5cCAkxX3qHhJifD9dT6GguNm3aNHyLfVP7+voyzYXf1J5Klj8A4UqpZkqpGsBQzOPBqzafmo6Ve4HhwyE93XyPMj1dJ0rNPYYPH87cuXMJCQlBKUVISAhz585luAv/AD22uqNSqj/wHuaW8YUiUuZXQpVY3bHwnmXRS3FVDRq08NpGHk2r6Oxt4DG4I5jSiMgaYI2nfr9XKkyIFaQ1XNOqEo8lS82GOoE6OWqaF/Lq1nBNqwqcNT/q7NmziYqKIjo6mmHDhnHz5k3AfH+vVatWREdHM2rUKO7cueO+ylUiOllqmoc5Y37U06dP88EHH7Br1y4OHDhAXl4ey5YtA8zJ8vDhw+zfv58bN254fLKTikonS03zMGct/WEymbhx4wYmk4nr16/TuLF5Wrr+/ftbZsLq2LGjZQZyzTE6WWqaF3J0ftTg4GAmTJhA06ZNadSoEQ0aNKBPnz5W+9y5c4dFixbRr18/l8VdmelkqWlezp75US9dukRycjInTpzgzJkzXLt2zTJBc6Fx48bRrVs3HnroIVeGW2npZKlpXqis2cRLs379epo1a0ZAQADVq1dn4MCBfP/995bX33jjDbKyspg1a5ZL467MdLLUKiVHWpiXLFlCbGwsMTExdO3albS0NMt7QkNDiYmJIS4ujvj4X/stDxkyhLi4OOLi4ggNDSUuLs6p8Tu69EfTpk3ZsWMH169fR0RISUmxLIQ3f/58vv32W5YuXVpidU7NAfbM4+YNm7ctWKZ5t02bNsnu3butJmieOHGi1QTNkyZNEhGRbdu2ycWLF0VEZM2aNdKxY0fLe0JCQiQrK6vM3/WnP/3JavZuRzlrftTXX39dWrVqJVFRUfLss89a5kH18fGR5s2bW2Ygv5dYKyO8efLf8mw6WWqOcnTpDxGRixcvSuPGjS3P75Ys8/PzxWg0yk8//eTEyJ1nsYiEiIgqeNST2Jdkb7LU5+RalVGe9aqVUvTp04f27dszd+7cEvtv2bKFBx54gPDwcNcFXk5LgDFABuapxDMKni/xZFAVmB7uqFVJpbUwf/fddyxYsICtW7dayrZu3UpwcDDnz5+nd+/eRERE0K1bN8vrS5cuZdiwYW6L2xGTgevFyq4XlOvJoRynzyy1cnGkAUVEeOmllwgLCyM2NpY9e/ZY3nPy5En69OlDZGQkrVu3tiwJkJiYSJs2bYiNjWXw4MFcvXr1nmMuz3rVhetQBwYG8uSTT7Jz507LayaTiZUrVzJkyJB7js0VbE0Z7Z1TSXs/nSy1cnFkiN7XX3/N0aNHOXr0KHPnzmXs2LGW94wYMYKJEyfy448/snPnTksCmz17Nmlpaezbt4+mTZsyZ86ce47Z0fWqr127xpUrVyw/r1271urLYf369URERGA0Gu85NlewNWe4XvWjnOy5sekNm27g8T72NqCMGTNG/vnPf5bY7+DBg/L//t//K/N35OfnywsvvCBvvfWWQ7E5Y73q48ePS2xsrMTGxkrr1q3lzTfftPodI0eOlI8//tihuNxpsYj4ivUHyVd0I09x6NZwzdWKJ8sGDRpYfs7Pz7c8f+yxx2TLli2W13r06CE//PCDfPHFF/LYY4/Jk08+KXFxcTJhwgQxmUyW/X73u99JYGCgdO/eXa5du+aGGtnpTq7IzXSRm8fMj3dyPR2RTbo1/O7sTZb6MlxzCXuG6JlMJrZs2cI777zDDz/8wM8//8zf//53y+uffPIJZ86cITIykuXLl7s4YjuZrkBeFmAqLDA/N13xZFQ2DQfSgfyCR92wU346WWpOY6sBJTg4mFOnfl35ODMzk+DgYIxGI3FxcTRv3hyDwcATTzxh1fgD4OPjw9ChQ/n888/dV5Gy5F3E3BGnKCko1yoznSw1p7HVgDJgwAD+8Y9/ICLs2LGDBg0a0KhRIzp06EBOTg6Fyxxv2LCB1q1bIyIcO3YMMN8mWrVqFREREZ6pVAkmB8u1SsOea3Vv2KrSPcuEhAQJCAiwuh+YnZ0tvXr1krCwMOnVq5dleN7ixYslJiZGoqOjpUuXLpKammp5z6VLl2TQoEHSqlUriYiIkO+//15ERFJTU6Vz584SHR0tv/nNbyzD5RzhSANKfn6+jBs3Tpo3by7R0dHyww8/WI6zdu1aS/wjR46UW7duSV5ennTt2lWio6MlKipKnnnmmXLF6BKWe5XFt3RPR6aVE7qBp+Jy1rjmESNGyLx580RE5NatW3Lp0iUREYmPj5eNGzeKiMiCBQvkP//zP11fqcriTq7IzePFEuVxr27k0cqmk2UFd6/jmnNyciQ0NFTy8/NL7Fe/fn1L+cmTJyUyMtIVVSi3NFkssyRE/ipKZkmIpHlbG24Fag3X7s7eZKnvWVYQjo5rPnHiBAEBASQkJNC2bVtGjx7NtWvXAIiKiiI5ORmAf/3rX1aNL562jyX8mzFcLhjRfJkM/s0Y9nnTiGZDPagZAjVbmB8N9TwdkeYGOllWQGWNa54xYwZg7pazZ88exo4dy969e6lTp45lRM3ChQv56KOPaN++PVeuXKFGjRpur4MtKUzmTrERzXe4TgqTPRSRppnpZFlBODqu2Wg0YjQa6dSpEwCDBw+2dMuJiIhg7dq17N69m2HDhtGiRQs318a2yzZGLtsq1zR30cmygnB0XHNQUBBNmjThyJEjAKSkpNC6dWsAzp8/D0B+fj5vvvkmL7zwgjurUqYGNkYu2yrXNHfRydILDRs2jC5dunDkyBGMRiMLFiwgKSmJdevWER4ezvr160lKSgJg6tSpZGdnM27cuBJLH/z3f/83w4cPJzY2ltTUVP7yl78A5mnFWrZsSUREBI0bNyYhIcEj9SxNT6ZRHV+rsur40pNpHoqoanNkdqnDhw/TpUsXatasyTvvvGN1nG+++YZWrVoRFhZmuR0E5nvrnTp1IiwsjCFDhnD79m33VKw87GkF8oatvK3hjvRZzM/Plz/84Q/SokULiYmJkd27d1vek5GRIb1795aIiAiJjIyUEydOiIhISkqKtG3bVqKiomTEiBFy586dcsXpLtuP3JRJn16S0R9my6RPL8n2Izc9HVIJXt8aXoU40o3t3LlzsnPnTvnLX/4ib7/9tmV/k8kkzZs3l+PHj8utW7ckNjZWDh48KCIiTz31lCxdulRERJ5//nn56KOP3FU1C3RruJkrpxLLz89n5MiRLFu2jAMHDhASEmK5VPZGO366xaKN17h4NR+Ai1fzWbTxGjt+uuXhyKzFMpzxpDOFfMaTTqwe0ewx3bp1w9/f36osOTmZkSNHAjBy5Ei+/PJLwDznZ4cOHahevbrV/jt37iQsLIzmzZtTo0YNhg4dSnJyMiLChg0bGDx4cIljeaNKnywd+c9OTk5mxIgRKKXo3LkzOTk5nD17lkOHDmEymejduzcAdevWxdfXl+zsbGrUqGG5V9i7d2/vGcNcii923OB2sVF5t03mck2zlz3d2Io6ffo0TZo0sTw3Go2cPn2a7OxsGjZsiMFgsCr3VpU+WZbG1n+2rf/Un376iYYNGzJw4EDatm3LxIkTycvL4/7778dkMrFr1y4APvvsM6/qs1hc4RmlveWadjf2zC5VWbgsWSqlpiilTiulUgu2/kVee1UpdUwpdUQp1ddVMdgZZ7mnElNKsWzZMsaPH0/Hjh2pV68ePj4+borccf51S//vtlWuaaUpqxtbaWzNOnXfffeRk5ODyWSyKvdWrv6UzBaRuIJtDYBSqjUwFIgC+gEfKaXcmmGcOZVYly5d2LJlCzt37qRbt25W3Xe8zZOda1Oj2BJ1NQzmck2zl61ubLZ06NCBo0ePcuLECW7fvs2yZcsYMGAASikeeeQRPvvsM7uP5UmeOKV4HFgmIrdE5ARwDOjozgCcNZUY/Npn8datW8yYMcOr+iwW17llTf6jex3LmaR/3Wr8R/c6dG5Z08ORad7KkW5sv/zyC0ajkVmzZvHmm29iNBrJzc3FYDAwZ84c+vbtS2RkJE8//TRRUVEAzJgxg1mzZhEWFkZ2djaJiYmerG7Z7GkyL88GTME8OfM+YCHgV1A+B3i2yH4LgMF3O155uw65cioxEZEJEyZIRESEtGzZUmbPnl2uGDWtKkpPT5d///vfsnz5cvn3v/8t6enpHokDO7sOKfO+5aOUWg8ElfLSZGAHcAHztNL/BTQSkVFKqTnADhFZXHCMBcDXIvJZKccfg3ldeJo2bdo+IyOj3LG6whLMFT2JecW8aehp+6uSUaNGsXr1agIDAzlw4ABg7rA9ZMgQ0tPTCQ0NZcWKFfj5+bFkyRJmzJiBiFCvXj0+/vhj2rRpY/M4AFOmTGHevHkEBAQA8Le//Y3+/fuXDKQCysjIYNeuXeTl5VnKfHx8iI+PJyQkxK2xKKV2i0j83fa7p8twEeklItGlbMkick5E8kQkH5jHr5fap4EmRQ5jLCgr7fhzRSReROIL/2C8xRLMWdw8N475cUxBuVY1ONKHt1mzZmzatIn9+/fz2muvMWbMmDKPU2j8+PGkpqaSmppaaRIlwP79+60SJUBeXh779+/3UER358rW8EZFnj4JFH5lrgKGKqVqKqWaAeHAzuLv93aTodjcOObnem6cqsORPrxdu3bFz88PgM6dO5OZmVnmcSq769eLf3rKLvcGrmzgmamU2q+U2gc8AowHEJGDwArgEPAN8HsRybN9GO9kaw4cPTdO1ebovKN3M2fOHGJjYxk1apRlDHZl4Ovr61C5N3BZshSR/xCRGBGJFZEBInK2yGvTRKSFiLQSka9dFYMr2ZoDR8+NoxWyZ97RsowdO5bjx4+TmppKo0aNeOWVV1wVqtvFxMSU6JPs4+NDTEyMhyK6O90buZymAcW/A30LyrWqy9F5R+92LB8fH6pVq8Zzzz3Hzp0V7m6VTSEhIcTHx1vOJH19fT3SuOMIw9130UpT2OqtW8O1ogr78CYlJdk172hZzp49a7mk/+KLL6ymSasMQkJCvDo5lmBP/yJv2KragmWa93OkD29iYqI0bNhQ2rRpI23atJGif8+lHUdE5Nlnn5Xo6GiJiYmR3/72t5YF6zTnwh39LN0pPj5eCies0LTK7s6dK5hMFxExoZQBg8Gf6tX1wmiuYG8/S30Zrmle5s6dK9y5k4W5By+ImAqeoxOmB+kGHk3zMibTRQoT5a+koFzzFJ0sNa/hrPVecnJyGDx4MBEREURGRrJ9+3YAhgwZQlxcHHFxcYSGhhIXF+e+yjlAxORQueYeOllqXsOR4YP+/v588MEHTJgwocRxXn75Zfr168fhw4dJS0sjMjISgOXLl1uGDg4aNIiBAwe6vlLloFTpd8dslWvuoZOl5jWcsd7L5cuX2bx5s2Wqrxo1atCwYUOrfUSEFStWMGzYMFdV5Z4YDP5A8QmpVUG55ik6WWpezdH1Xk6cOEFAQAAJCQm0bduW0aNHc+3aNat9tmzZwgMPPEB4eLjL4r4X1avXo3r1AMuZpFIGqlcP0I07HqaTpVZh2LsEyJ49exg7dix79+6lTp06VutUg3nddG89qyxUvXo9atcOwde3BbVrh+hE6QV0stS8mqPrvRiNRoxGI506dQJg8ODBliVAwJxMV65cyZAhQ1wXtFYp6WSpeTVH13sJCgqiSZMmHDlyBICUlBTLEiAA69evJyIiAqPR6LqgtcrJnmE+3rDp4Y7ll5CQIAEBARIVFWUpy87Oll69eklYWJj06tVLLl68KCIiP/74o3Tu3Flq1Kghb7/9doljmUwmiYuLk8cee8xS9swzz0jLli0lKipKEhIS5Pbt2+WK05Hhg2fPnpXg4GCpV6+eNGjQQIKDg+Xy5csiIrJ3715p3769xMTEyOOPP26pm4jIyJEj5eOPPy5XfFrlhB7uqBXavHkzdevWZcSIEZZlCyZNmoS/vz9JSUm89dZbXLp0iRkzZnD+/HkyMjL48ssv8fPzK9E1Z9asWVNKLq4AABBYSURBVOzatYvc3FxWr14NwJo1ayzzMz7zzDN069aNsWPHureSmlZObllWQqsYnNElB8xLA3/11VeMHj3aqrx///6WxpeOHTtazQLuLQ6tvsb/9j7D2zGn+N/eZzi0+trd36RpRehkWUU52iUH4I9//CMzZ86kWrXS/2zu3LnDokWL6Nevn1NjvVeHVl/j2ymXyD2bBwK5Z/P4dsolnTA1h+hkqdnVJadw9cH27dvb3GfcuHF069aNhx56yNkh3pMt71/GdNP6dpPpprDl/cseikiriHSyrKIc7ZKzbds2Vq1aRWhoKEOHDmXDhg08++yzltffeOMNsrKymDVrlkvjLo/cX0pf4slWuaaVRifLKsrRLjnTp08nMzOT9PR0li1bRo8ePVi8eDEA8+fP59tvv2Xp0qU2L9E9qX6Qj0PlmlYa7/vL1pxu2LBhdOnShSNHjmA0GlmwYAFJSUmsW7eO8PBw1q9fT1JSEgC//PILRqORWbNm8eabb2I0GsnNzS3z+C+88ALnzp2jS5cuxMXFMXXqVHdUy24PvdwAQy3r2wyGWoqHXm7goYi0ikh3HdKqhEOrr7Hl/cvk/pJH/SAfHnq5Aa1/U8fTYWleQM+UrrmP6QrkXQRMgAF8/MHgXWOZW/+mjk6O2j3RyVK7N6YrkPfrEghgKniO1yVMTbsX+p6ldm/ySl8CwVyuaZWHTpbaPbK11IFeAkGrXHSy1O6RrTs5+g6PVrnoZKndG5/Sl0Awl2vu5KwF30o7DsCUKVMIDg62LPq2Zs0a11fKi+hkqd0bQz3wCeDXM0mD+blu3HE7Zy34VtpxCo0fP96y6Fv//v2dXwkvppOldu8M9aBmCNRsYX7UidIjnDW7VGnH0XSy1LRKrTyzS5Vlzpw5xMbGMmrUKMslfVWhk6WmVRH2zC5VlrFjx3L8+HFSU1Np1KgRr7zyihOj8373lCyVUk8ppQ4qpfKVUvHFXntVKXVMKXVEKdW3SHm/grJjSqmke/n9mqaVzdHZpe52LB8fH6pVq8Zzzz3Hzp07nRVmhXCvZ5YHgIHA5qKFSqnWwFAgCugHfKSU8lFK+QAfAo8CrYFhBftWCo60RooIL730EmFhYcTGxlqtQOjj42NpcRwwYIClPDExkTZt2hAbG8vgwYO5evWq+yqnVUiOzi5VlsKkC/DFF1+UaC2v9OxZqOduG7ARiC/y/FXg1SLPvwW6FGzf2tqvrK0iLFi2adMm2b17t9XCYBMnTpTp06eLiMj06dNl0qRJIiLy1VdfSb9+/SQ/P1+2b98uHTt2tLynTp06pR6/cEEuEZHx48dbjqtpIs5b8K2044iIPPvssxIdHS0xMTHy29/+Vs6cOeOxujoTdi5Y5qqew8HAjiLPMwvKAE4VK+9k6yBKqTHAGICmTZs6OUTn69atG+np6VZlycnJbNy4ETC3Rnbv3p0ZM2aQnJzMiBEjUErRuXNncnJyOHv2rOVmfGnq168PmL/gbty4cU/3n7TKZ+nSpaWWp6SklCgLCgqyuVZSacdZAmxZtIiTQFNgCGD7L7VyuutluFJqvVLqQClb+c/n7SQic0UkXkTiAwICXP3rXMJWa+Tp06dp0qSJZT+j0cjp06cBuHnzJvHx8XTu3NnS1aNQQkICQUFBHD58mD/84Q9uqoVWlS3BfMaSgXkWgIyC50s8GZQH3PXMUkR6leO4p4EmRZ4bC8ooo7zSs7c1MiMjg+DgYH7++Wd69OhBTEwMLVq0AOCTTz4hLy+PP/zhDyxfvpyEhARXh61VcZOB68XKrheUD3d/OB7jqq5Dq4ChSqmaSqlmQDiwE/gBCFdKNVNK1cDcCLTKRTF4BVutkcHBwZw69esdiczMTIKDgy2vATRv3pzu3buzd+9eq2P6+Pz/9u4/Nur6juP48w0IyKBsILKrjAGxtx4thEizUMIMmm1YE3CwLKJLJNlcA4yYGBfTBUwadUswmX8Q55ZuJeFXhjp0kAEKDFulyqRutLYyaCsM2yoFaqhEWzjuvT/u2+6Au/Z7Xu9732vfj+QbLp/v3fde9+2Xd+77/dzn+xnJypUr2bVrlxcfwQxzZ5NsH6pS/enQchFpJdpxs1dE3gBQ1UbgZeBD4HXgl6p6TVXDwDqiHT4ngJed5w5ZiXojly1bxtatW1FVjh49ysSJEwkEAnz22Wf09PQAcOHCBWpqapg9ezaqSnNzMxC9Zrlnzx7y8/Mz86HMsJKot8D/vQiDzE0vkB+WbOgNT6Y3MhKJ6Nq1a3XWrFlaWFiox44dU1XVmpoaLSws1Llz52phYWFfT+S1a9d04cKFWlhYqAUFBfrwww9f1ztuTLpsV9Vxev1/yHFO+1CAy95wm4PHGDOgHUSvUfb2hv+GoXO90ubgGWKudV8i8sV5iIRhxChGjJvCyLE2O6Hxxk8ZOsXxq7Kx4VngWvclIpc/jRZKgEiYyOVPudZ9KbPBhoDBGnX15JNPUlBQQCgU4rHHHuPGM7Zly5YNvxEvQ4wVyywQ+SJ2QrBe6rSbVCRzD8j9+/fT1NREU1MTFRUVrFmzBoB33nmHmpoa6uvraWho4NixY1RXV/dt79VXX2X8+PHefSiTFlYss0EkwXw2idqNa8ncAzLRqCsRobu7mytXrtDT08PVq1eZOnUqAJcvX+b5559nw4YN3n4wM+isWGaDEQkuLSdqNylJdtRVcXEx99xzD4FAgEAgwJIlSwiFQgA89dRTPPHEE4wbN877D2IGlRXLLDBi3BTizXMTbTfp5GbUVXNzMydOnKC1tZW2tjYOHz7M22+/zfHjx2lpaWH58uUepTXpZF9NskBvr7f1hnujd9RVIBBwNepq+/btLFiwoO+6ZElJCe+++y4TJkygtraWGTNmEA6H6ejoYPHixX03VjHZxb5ZZomRYydyy6Q7ueW2fG6ZdKfvCmUyvco7duxg7ty5zJkzh4ULF1JXV9fvdgDq6uooLi5mzpw5LF26lK6urrR9lmRHXU2fPp3q6mrC4TBXr16lurqaUCjEmjVraG9v58yZMxw5coRgMGiFMpu5+eW6H5ZsGMEznCVzL8+amhrt7OxUVdV9+/Zddy/PeNtRVS0qKtKqqipVVa2srNQNGzYMSu7BGHUVDoe1tLRU8/PzNRQK6eOPP37T+5w+ffqmz2T8AZcjeDJeBN0uViz978aCEAwG+24Q297ersFg8KbXdHZ2am5ubr/bUVXNycnRSCSiqqpnz57VUCg02PEHXVvDTq16IV9f/+14rXohX9sadmY6konDbbG003CTNm5mFqysrKSkpGTAbRUUFLB7924AXnnlleuuHfpRe+NLNO5bR3fXx4DS3fUxjfvW0d74Uqajma/IiqXxRLxe5TfffJPKyko2btw44Os3b97Miy++yPz58/n8888ZPXp0uqIOiqaqciLhL69ri4S/pKmqPDOBTMqsN9ykTaJeZYD6+noeffRR9u/fz+TJkwfcVn5+PgcOHADg1KlT7N27N225B0N3V/wpGxK1G/+zb5YmbRL1Kp89e5YVK1awbds2gsGgq211dHQAEIlEePbZZ1m9enV6Qg+SsTnTkmo3/mfF0gyKhx56iOLiYk6ePMm0adOorKykrKyMgwcPkpeXx6FDhygri04T//TTT3Px4kXWrl3LvHnzKCoq6nc7EJ1EKxgMkp+fT25uru+n08hbXM6IUbde1zZi1K3kLS7PTCCTMrufpTFp0t74Ek1V5XR3tTI2Zxp5i8vJLXgw07HMDex+lmZIOXzwQ7ZUHOF8RxdTbs9hVeki7v3B7EzH6lduwYNWHIcQK5bG9w4f/JBNzx2gpyd6l6WOc11sei7a2eP3gmmGDrtmaXxvS8WRvkLZq6cnzJaKIxlKZIYjK5bG9853xB8HnqjdmHSwYml8b8rtOUm1G5MOViyN760qXcSYMddfXh8zZhSrShdlKJEZjqyDx/hebydOtvWGm6HFiqXJCvf+YLYVR5NRdhpujDEuWLE0xhgXrFgaY4wLViyNMcYFK5bGGOOCFUtjjHEhpWIpIj8RkUYRiYhIUUz7DBH5UkSOO8sfY9bNF5EPRKRZRDbJQDPYG2OMD6T6zbIBWAG8FWddi6rOc5bY21r/AfgFkOcs96WYwRhj0i6lYqmqJ1T1pNvni0gAyFHVo84UlFuBH6WSwRhjvJDOa5YzReTfIlItIt9z2u4AYmdsanXa4hKRUhGpFZHa8+fPpzGqMcb0b8DhjiJyCPhmnFXrVXV3gpd9AkxX1YsiMh/4m4gUJBtOVSuACifHeRH5b7LbcOE24EIatpsqv+YC/2azXMnzazYvc33bzZMGLJaq+v1k31lVe4Ae5/H7ItICBIE2IHZ6u2lOm5ttTkk2hxsiUutm/g2v+TUX+Deb5UqeX7P5MVdaTsNFZIqIjHQezyLakfORqn4CdInIAqcX/BEg0bdTY4zxjVR/OrRcRFqBYmCviLzhrLobqBeR48BfgdWq2umsWwv8GWgGWoD9qWQwxhgvpHSLNlV9DXgtTvsuYFeC19QCham87yCryHSABPyaC/ybzXIlz6/ZfJcra+YNN8aYTLLhjsYY44IVS2OMcWHYFEs/j2NPlM1Z92vn/U+KyJKY9vuctmYRKUtHrhtylItIW8x+un+gjF7xel+4yHPGOW6Oi0it0zZJRA6KSJPz7zc8yLFZRDpEpCGmLW4Oidrk7MN6EbnL41y+Pb76qOqwWIAQ8B2gCiiKaZ8BNCR4zXvAAkCI9tqXeJxtNlAHjAFmEv31wEhnaQFmAaOd58xO8/4rB34Vpz1uRg//rp7vCxeZzgC33dD2HFDmPC4DNnqQ427grtjjO1EO4H7nGBfnmP+nx7l8eXzFLsPmm6X6eBx7P9keAHaqao+qnib6c6vvOkuzqn6kqleAnc5zMyFRRq/4aV/05wFgi/N4Cx7cE0FV3wI6b2hOlOMBYKtGHQW+7vwf8CpXIpk+vvoMm2I5gJTHsafJHcDHcTIkak+3dc4p2uaY08hMZemV6fePR4EDIvK+iJQ6bVM1OigD4FNgamaiJczhh/3ox+Orz5CaCjeT49jTlM1T/WUkemu9Z4gWgmeA3wE/8y5dVlmkqm0icjtwUET+E7tSVVVEMv6bPb/kcPj++BpSxVJ9Mo59sLI57/etBBkStX9lbjOKyJ+Av7vI6IVMv/9NVLXN+bdDRF4jetp4TkQCqvqJc3rbkaF4iXJkdD+q6rnexz47vvoM+9Nwn49j3wOsFJExIjLTyfYecAzIE5GZIjIaWOk8N21uuH61nOiNn/vL6BXP90V/RORrIjKh9zHwQ6L7ag+wynnaKjJ3T4REOfYAjzi94guASzGn62nn4+Pr/zLRq5SJxfkDtBL9FnkOeMNp/zHQCBwH/gUsjXlNEdE/WgvwAs6IJ6+yOevWO+9/kpjeeKK9l6ecdes92H/bgA+AeqIHcGCgjB7+bT3dFwNkmUW097bOOa7WO+2TgX8ATcAhYJIHWf5C9DLTVef4+nmiHER7wX/v7MMPiPlVhke5fHt89S423NEYY1wY9qfhxhjjhhVLY4xxwYqlMca4YMXSGGNcsGJpjDEuWLE0xhgXrFgaY4wL/wO+F7y+nFDafwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1145c37f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pylint: disable=missing-docstring\n",
    "# Function to draw visualization of distance between embeddings.\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 5\n",
    "fig_size[1] = 5\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "tsne_seed = 123\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    #   plt.figure(figsize=(18, 18))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y, color=node_colors[(node_labels[label-1]-1)])\n",
    "        print(str(label) + '->' + str(node_labels[label-1]))\n",
    "        plt.annotate(\n",
    "            label,\n",
    "            xy=(x, y),\n",
    "            xytext=(5, 2),\n",
    "            textcoords='offset points',\n",
    "            ha='right',\n",
    "            va='bottom')\n",
    "\n",
    "    plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "#     print(final_embeddings)\n",
    "    tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000, method='exact', random_state=tsne_seed)\n",
    "    plot_only = 34\n",
    "    nodes_to_plot = curr_vocab[len(curr_vocab)-plot_only:len(curr_vocab)]\n",
    "#     low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "    print(len(final_embeddings[nodes_to_plot-1]))\n",
    "    print(len(final_embeddings))\n",
    "    low_dim_embs = tsne.fit_transform(final_embeddings[nodes_to_plot-1])\n",
    "#     labels = [(i+start_id) for i in range(plot_only)]\n",
    "    labels = nodes_to_plot\n",
    "    plot_with_labels(low_dim_embs, labels, os.path.join(gettempdir(), 'tsne.png'))\n",
    "\n",
    "except ImportError as ex:\n",
    "    print('Please install sklearn, matplotlib, and scipy to show embeddings.')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1    21]\n",
      " [    2     8]\n",
      " [    3     6]\n",
      " ...\n",
      " [10310    29]\n",
      " [10311    22]\n",
      " [10312    16]]\n",
      "[    1     3     9 ... 10297 10298 10303]\n",
      "1928\n"
     ]
    }
   ],
   "source": [
    "def read_labels(fname):\n",
    "    rws = np.concatenate([\n",
    "        np.loadtxt(f.open(), delimiter='\\t', usecols=(0, 1), dtype=int) # accepts only the first label\n",
    "        for f in l_location.glob(fname)\n",
    "        if f.stat().st_size > 0\n",
    "    ])\n",
    "\n",
    "    print(rws)\n",
    "    return (rws[:,0],rws[:,1])\n",
    "\n",
    "def read_vocabs(fname):\n",
    "    rws = np.concatenate([\n",
    "        np.loadtxt(f.open(), dtype=int) # accepts only the first label\n",
    "        for f in l_location.glob(fname)\n",
    "        if f.stat().st_size > 0\n",
    "    ])\n",
    "\n",
    "    print(rws)\n",
    "    return rws\n",
    "\n",
    "l_location = Path(\"/Users/Ganymedian/Desktop/dynamic-rw/output/blog-catalog/\")\n",
    "lf_name = \"sorted-labels.txt\"\n",
    "vocab_file = \"gPairs-vocabs-w4-s8.txt\"\n",
    "v_labels = read_labels(lf_name)\n",
    "curr_vocab = read_vocabs(vocab_file)\n",
    "nodes = v_labels[0]\n",
    "print(len(curr_vocab))\n",
    "node_labels = v_labels[1]\n",
    "# node_labels = node_labels[curr_vocab-1]\n",
    "node_colors = []\n",
    "\n",
    "import matplotlib\n",
    "for name, hex in matplotlib.colors.cnames.items():\n",
    "    node_colors.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "colors = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_seed= 123\n",
    "def class_train_test(vectors, train_size, test_size):\n",
    "# Split using sklearn ShuffleSplit\n",
    "    ss = model_selection.ShuffleSplit(n_splits=1,\n",
    "                                      train_size=train_size,\n",
    "                                      test_size=test_size,\n",
    "                                        random_state = r_seed)\n",
    "    train_index, test_index = next(ss.split(vectors))\n",
    "\n",
    "    train_data = vectors[train_index]\n",
    "    test_data = vectors[test_index]\n",
    "    train_labels = node_labels[train_index]\n",
    "    test_labels  = node_labels[test_index]\n",
    "    # print(train_labels)\n",
    "    # print(test_labels)\n",
    "\n",
    "    # Classifier choice\n",
    "    #classifier = linear_model.LogisticRegression(C=10)\n",
    "    classifier = svm.SVC(C=1)\n",
    "\n",
    "    clf = make_pipeline(preprocessing.StandardScaler(), classifier)\n",
    "    clf.fit(train_data, train_labels)\n",
    "\n",
    "    train_pred = clf.predict(train_data)\n",
    "    test_pred = clf.predict(test_data)\n",
    "    # print(train_pred)\n",
    "    # print(test_pred)\n",
    "    train_acc = clf.score(train_data, train_labels)\n",
    "    test_acc = clf.score(test_data, test_labels)\n",
    "    train_f1 = f1_score(train_labels, train_pred, average='micro')\n",
    "    test_f1 = f1_score(test_labels, test_pred, average='micro')\n",
    "#     print(\"Train acc:\", train_acc)\n",
    "#     print(\"Test acc:\", test_acc)\n",
    "#     print(\"Train f1:\", train_f1)\n",
    "#     print(\"Test f1:\", test_f1)\n",
    "    return {'train_acc':train_acc, 'test_acc':test_acc, 'train_f1':train_f1, 'test_f1':test_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.3\n",
    "test_size = 1-train_size\n",
    "lrs = class_train_test(final_embeddings, train_size, test_size)\n",
    "print(lrs.get('test_f1'))\n",
    "print(lrs.get('train_f1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
